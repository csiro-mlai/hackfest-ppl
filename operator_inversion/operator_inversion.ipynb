{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference in a PDE model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look, we can do approximate probabilistic inference with a complicated neural network!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt, ceil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "import torch.fft as fft\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.modules.loss import MSELoss\n",
    "\n",
    "from src.nn_modules.fourier_2d_generic import SimpleBlock2dGeneric\n",
    "from src.heatmap import multi_heatmap\n",
    "from src.utils import resolve_path\n",
    "\n",
    "# device = torch.device('cuda')\n",
    "device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is packed as as $(b, x, y, t)$ i.e. batch first, coordinates in the middle, time last. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = h5py.File(resolve_path('./data/grf_forcing_pico.h5'))\n",
    "data = data_file['valid']\n",
    "\n",
    "def get_obs(data, t=0, n_steps=2, y=False):\n",
    "    x = data['u'][..., t:t+n_steps]\n",
    "    latent = data['f'][...]\n",
    "    obs= {\n",
    "        'x': x,\n",
    "        'latent': latent\n",
    "    }\n",
    "    if y:\n",
    "        obs['y'] = data['u'][..., t+n_steps]\n",
    "\n",
    "    return obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_as_tensor(d, device=device):\n",
    "    \"\"\"\n",
    "    it was faster to write this function than to search the docs for it\n",
    "    \"\"\"\n",
    "    return {k: torch.from_numpy(v).to(device) for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def multi_img_time(x, batch=0, interval=1, n_cols=None, fsize=6):\n",
    "    \"\"\"\n",
    "    Plot multiple timesteps of an array (time last)\n",
    "    \"\"\"\n",
    "    steps = range(0, x.shape[-1], interval)\n",
    "    if n_cols is None:\n",
    "        n_cols = len(steps)\n",
    "    print(steps, len(steps), n_cols)\n",
    "    n_rows = ceil(len(steps) / n_cols)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(fsize * n_cols/n_rows, fsize));\n",
    "    axes = axes.flatten()\n",
    "    for ax in axes:\n",
    "        ax.set_axis_off()\n",
    "    for i, ax in zip(steps, axes):\n",
    "        ax.imshow(x[batch,..., i])\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def multi_img_batch(x, interval=1, n_cols=None, fsize=6):\n",
    "    \"\"\"\n",
    "    Plot multiple batches of an array \n",
    "    \"\"\"\n",
    "    steps = range(0, x.shape[0], interval)\n",
    "    if n_cols is None:\n",
    "        n_cols = len(steps)\n",
    "    n_rows = ceil(len(steps) / n_cols)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(fsize * n_cols/n_rows, fsize));\n",
    "    axes = axes.flatten()\n",
    "    for ax in axes:\n",
    "        ax.set_axis_off()\n",
    "    for i, ax in zip(steps, axes):\n",
    "        ax.imshow(x[..., i])\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = get_obs(data, 0, 8)['x']\n",
    "\n",
    "multi_img_time(arr, n_cols=2);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have a process predictor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_state_dict = torch.load(\n",
    "    './models/fno_forward.ckpt',\n",
    "    map_location=device\n",
    ")\n",
    "process_predictor = SimpleBlock2dGeneric(\n",
    "    modes1=16,\n",
    "    width=24,\n",
    "    n_layers=4,\n",
    "    n_history=2,\n",
    "    param=False,\n",
    "    forcing=False,\n",
    "    latent=True,\n",
    ")\n",
    "process_predictor.load_state_dict(\n",
    "    pp_state_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_forward(x, latent, n_horizon=1, n_steps=2):\n",
    "    x = torch.as_tensor(x).to(device)\n",
    "    latent = torch.as_tensor(latent).to(device)\n",
    "    for i in range(n_horizon):\n",
    "        pred = process_predictor({'x': x[...,-(n_steps):], 'latent': latent})['forecast']\n",
    "        x = torch.cat((x, pred), dim=-1)\n",
    "    return x[...,-n_horizon:]\n",
    "\n",
    "obs = get_obs(data, 0, 10)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = predict_forward(obs['x'], obs['latent'], n_horizon=100, n_steps=2)\n",
    "\n",
    "multi_img_time(pred.cpu().numpy(), n_cols=5, interval=5);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_img_time(pred.cpu().numpy(), n_cols=5, interval=5);\n",
    "plt.savefig('./fno_forward_predict_sheet.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inversion by GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RasterLatent(nn.Module):\n",
    "    def __init__(self,\n",
    "            process_predictor: \"nn.Module\",\n",
    "            dims = (256,256),\n",
    "            n_batch: int=1):\n",
    "        super().__init__()\n",
    "        self.dims = dims\n",
    "        self.process_predictor = process_predictor\n",
    "        ## Do not fit the process predictor weights\n",
    "        for param in self.process_predictor.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.latent = nn.Parameter(\n",
    "            torch.zeros(\n",
    "                (n_batch, *dims),\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def weights_init(self):\n",
    "        self.latent.data.normal_(0.0, 0.01)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        #copy\n",
    "        batch = dict(**batch)\n",
    "        batch['latent'] = self.latent\n",
    "        return self.process_predictor(batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "        batch,\n",
    "        model,\n",
    "        optimizer,\n",
    "        n_iter: int=20,\n",
    "        check_int: int=1,\n",
    "        clip_val = None,\n",
    "        callback = lambda *x: None):\n",
    "    model.train()\n",
    "    model.weights_init()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    big_loss_fn = nn.MSELoss(reduction='none')\n",
    "    scale = loss_fn(torch.zeros_like(batch['latent']), batch['latent']).item()\n",
    "    for i in range(n_iter):\n",
    "        # Compute prediction error\n",
    "        pred = model(batch)\n",
    "        loss = loss_fn(pred['forecast'], batch['y'])\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if clip_val is not None:\n",
    "            for group in optimizer.param_groups:\n",
    "                torch.nn.utils.clip_grad_value_(group[\"params\"], clip_val)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % check_int == 0 or i==n_iter-1:\n",
    "            with torch.no_grad():\n",
    "                loss_v = loss.item()\n",
    "                # batchwise error\n",
    "                big_loss_v = big_loss_fn(pred['forecast'], batch['y']).mean(dim=(1,2,3))\n",
    "                big_error = big_loss_fn(model.latent, batch['latent']).mean(dim=(1,2))\n",
    "                big_relerr = torch.sqrt(big_error/scale)\n",
    "                error = big_error.mean().item()\n",
    "                relerr = sqrt(big_relerr.mean().item())\n",
    "                print(\n",
    "                    f\"loss: {loss:.3e}, error: {error:.3e}, relerror: {relerr:.3e} [{i:>5d}/{n_iter:>5d}]\")\n",
    "\n",
    "                target =  batch['latent'][0, :, :].cpu().numpy()\n",
    "                est =  model.latent[0, :, :].cpu().numpy()\n",
    "                err_heatmap = target - est\n",
    "                fig = multi_heatmap(\n",
    "                    [target, est, err_heatmap],\n",
    "                    [\"Target\", \"Estimate\", \"Error\"])\n",
    "                plt.show();\n",
    "                plt.close(\"all\");\n",
    "\n",
    "    return loss_v, error, relerr, scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RasterLatent(\n",
    "    process_predictor,\n",
    "    dims=obs['x'].shape[1:3],\n",
    "    n_batch=1)\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0025,\n",
    "    weight_decay=0.0)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "fit(\n",
    "    dict_as_tensor(get_obs(data,t=0,n_steps=2,y=True)),\n",
    "    model,\n",
    "    optimizer,\n",
    "    n_iter=50,\n",
    "    check_int=10,\n",
    "    clip_val=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RasterLatent(\n",
    "    process_predictor,\n",
    "    dims=obs['x'].shape[1:3],\n",
    "    n_batch=1)\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.00025,\n",
    "    weight_decay=10)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "fit(\n",
    "    dict_as_tensor(get_obs(data,t=0,n_steps=2,y=True)),\n",
    "    model,\n",
    "    optimizer,\n",
    "    n_iter=50,\n",
    "    check_int=10,\n",
    "    clip_val=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic version\n",
    "\n",
    "* [Modules in Pyro — Pyro Tutorials](https://pyro.ai/examples/modules.html)\n",
    "* [Neural Networks — Pyro documentation](https://docs.pyro.ai/en/stable/nn.html#pyro.nn.module.PyroSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "from pyro.nn import PyroModule, PyroParam, PyroSample\n",
    "from pyro.nn.module import to_pyro_module_\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO, Predictive, MCMC, NUTS\n",
    "from pyro import poutine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbRasterLatent(PyroModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            process_predictor: \"nn.Module\",\n",
    "            dims = (256,256),\n",
    "            prior_scale = 0.01,\n",
    "            obs_scale = 0.01,):\n",
    "        super().__init__()\n",
    "        self.dims = dims\n",
    "        self.prior_scale = prior_scale\n",
    "        self.obs_scale = obs_scale\n",
    "        self.process_predictor = process_predictor\n",
    "        process_predictor.train(False)\n",
    "        ## Do not fit the process predictor weights\n",
    "        for param in self.process_predictor.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.latent = PyroSample(dist.Normal(0, 0.01).expand(dims).to_event(2))\n",
    "\n",
    "    def forward(self, X, y=None):\n",
    "        #overwrite process predictor batch with my own latent\n",
    "        mean = self.process_predictor({\n",
    "            'x': X,\n",
    "            'latent': self.latent.unsqueeze(0),\n",
    "        })['forecast']\n",
    "        return pyro.sample(\n",
    "            \"obs\", dist.Normal(mean, self.obs_scale).to_event(2),\n",
    "            obs=y)\n",
    "\n",
    "model = ProbRasterLatent(\n",
    "    process_predictor,\n",
    "    dims=obs['x'].shape[1:3],\n",
    "    prior_scale=0.01,\n",
    "    obs_scale=0.01,\n",
    ")\n",
    "\n",
    "nuts_kernel = NUTS(model, full_mass=False, max_tree_depth=5, jit_compile=True) # high performacne params\n",
    "\n",
    "mcmc = MCMC(nuts_kernel, num_samples=100, warmup_steps=100)\n",
    "obs = get_obs(data, 0, 2, y=True)\n",
    "mcmc.run(torch.as_tensor(obs['x']), torch.as_tensor(obs['y']))\n",
    "mc_samples = {k: v.detach().cpu().numpy() for k, v in mcmc.get_samples().items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"data/samples1.npz\", mc_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_img_batch(mc_samples['latent'], interval=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## offcuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fourier2dMapping(nn.Module):\n",
    "    \"\"\"\n",
    "    Does not work because I tried to do something fancy with parameterizations.\n",
    "    TODO.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, modes: int=20, dims=(256,256), prior_scale=0.01, obs_scale=0.01):\n",
    "        super().__init__()\n",
    "        self.modes = modes  # maybe just normalize the weights?\n",
    "        self.dims = dims\n",
    "        self.prior_scale = prior_scale\n",
    "        self.obs_scale = obs_scale\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        map from complex inputs on a half space to real inputs on a full space\n",
    "        \"\"\"\n",
    "        print(\"X\", X.shape, X.dtype)\n",
    "        return fft.irfft2(X, s=self.dims, norm=\"ortho\")\n",
    "\n",
    "    def right_inverse(self, Xp):\n",
    "        \"\"\"\n",
    "        map from real inputs on a full space to complex inputs on a half space\n",
    "        \"\"\"\n",
    "        return fft.rfft2(Xp, s=self.dims, norm=\"ortho\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "887521cc036c2176fc4c7c5fad660bcf5f9a9c2cadfc49851d28bf162a40070d"
  },
  "kernelspec": {
   "display_name": "Cadabra2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
