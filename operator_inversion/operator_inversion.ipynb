{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference in a PDE model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look we can do approximate probabilistic inference with a complicated nueral network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt, ceil\n",
    "from typing import Any, Dict, Tuple, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('png')\n",
    "# plt.rcParams.update({figure.figsize'=[12, 12]})\n",
    "# plt.rcParams.update({'figure.dpi': 200})\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "%matplotlib inline\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv());\n",
    "\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "import torch.fft as fft\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from src.nn_modules.fourier_2d_generic import SimpleBlock2dGeneric\n",
    "from src.heatmap import multi_heatmap\n",
    "from src.utils import resolve_path\n",
    "\n",
    "# device = torch.device('cuda')\n",
    "device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is packed as as $(b, x, y, t)$ i.e. batch first, coordinates in the middle, time last. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = h5py.File(resolve_path('./data/grf_forcing_pico.h5'))\n",
    "data = data_file['valid']\n",
    "\n",
    "def get_obs(data, t=0, n_steps=2):\n",
    "    x = data['u'][..., t:t+n_steps]\n",
    "    latent = data['f'][...]\n",
    "    return {\n",
    "        'x': x,\n",
    "        'latent': latent\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def multi_img_time(x, batch=0, interval=1, n_cols=None, fsize=6):\n",
    "    \"\"\"\n",
    "    Plot multiple timesteps of an array (time last)\n",
    "    \"\"\"\n",
    "    steps = range(0, x.shape[-1], interval)\n",
    "    if n_cols is None:\n",
    "        n_cols = len(steps)\n",
    "    print(steps, len(steps), n_cols)\n",
    "    n_rows = ceil(len(steps) / n_cols)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(fsize * n_cols/n_rows, fsize));\n",
    "    axes = axes.flatten()\n",
    "    for ax in axes:\n",
    "        ax.set_axis_off()\n",
    "    for i, ax in zip(steps, axes):\n",
    "        ax.imshow(x[batch,..., i])\n",
    "    plt.tight_layout()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = get_obs(data, 0, 10)['x']\n",
    "\n",
    "multi_img_time(arr, n_cols=2);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have a process predictor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_state_dict = torch.load(\n",
    "    './models/fno_forward.ckpt',\n",
    "    map_location=device\n",
    ")\n",
    "process_predictor = SimpleBlock2dGeneric(\n",
    "    modes1=16,\n",
    "    width=24,\n",
    "    n_layers=4,\n",
    "    n_history=2,\n",
    "    param=False,\n",
    "    forcing=False,\n",
    "    latent=True,\n",
    ")\n",
    "process_predictor.load_state_dict(\n",
    "    pp_state_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_forward(x, latent, n_horizon=1, n_steps=2):\n",
    "    x = torch.as_tensor(x).to(device)\n",
    "    latent = torch.as_tensor(latent).to(device)\n",
    "    for i in range(n_horizon):\n",
    "        pred = process_predictor({'x': x[...,-(n_steps):], 'latent': latent})['forecast']\n",
    "        x = torch.cat((x, pred), dim=-1)\n",
    "    return x[...,-n_horizon:]\n",
    "\n",
    "obs = get_obs(data, 0, 10)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = predict_forward(obs['x'], obs['latent'], n_horizon=100, n_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_img_time(pred.cpu().numpy(), n_cols=5, interval=5);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inversion by GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_heatmap(model, i, loss, error, loss_fn, batch, pred, *args, **kwargs):\n",
    "    target =  batch['latent'][0, :, :].cpu().numpy()\n",
    "    est =  model.latent[0, :, :].cpu().numpy()\n",
    "    err_heatmap = target - est\n",
    "\n",
    "    fig = heatmap.multi_heatmap(\n",
    "        [target, est, err_heatmap],\n",
    "        [\"Target\", \"Estimate\", \"Error\"], *args, **kwargs)\n",
    "    # plt.savefig(f\"paper_ml4ps/inverse_reg_{i}.png\",\n",
    "    #     dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "    # np.savez_compressed(f\"paper_ml4ps/inverse_reg_{i}.npz\",\n",
    "    #     target=target,\n",
    "    #     est=est,\n",
    "    #     error=err_heatmap\n",
    "    # )\n",
    "    plt.show();\n",
    "    plt.close(\"all\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveLatent(nn.Module):\n",
    "    def __init__(self,\n",
    "            process_predictor: \"nn.Module\",\n",
    "            dims: Tuple[int, int]=(256,256),\n",
    "            n_batch: int=1):\n",
    "        super().__init__()\n",
    "        self.dims = dims\n",
    "        self.process_predictor = process_predictor\n",
    "        ## Do not fit the process predictor weights\n",
    "        for param in self.process_predictor.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.latent = nn.Parameter(\n",
    "            torch.zeros(\n",
    "                (n_batch, *dims),\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def weights_init(self):\n",
    "        self.latent.data.normal_(0.0, 0.01)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        #copy\n",
    "        batch = dict(**batch)\n",
    "        batch['latent'] = self.latent\n",
    "        return self.process_predictor(batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "        batch,\n",
    "        model,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        n_iter:int=20,\n",
    "        check_int:int=1,\n",
    "        clip_val: Optional[float] = None,\n",
    "        callback = lambda *x: None,\n",
    "        # pen_0: float = 0.0,\n",
    "        pen_1: float = 0.0,\n",
    "        pen_f: float = 0.0,\n",
    "        stop_on_truth: bool = False,\n",
    "        diminishing_returns=1.1,):\n",
    "    model.train()\n",
    "    model.weights_init()\n",
    "    prev_loss_v = 10^5\n",
    "    prev_error = 10^5\n",
    "    prev_relerr = 10^3\n",
    "    big_losses = []\n",
    "    big_loss_fn = MSELoss(reduction='none')\n",
    "    big_scale = big_loss_fn(torch.zeros_like(batch['latent']), batch['latent']).mean((1,2))\n",
    "    scale = loss_fn(torch.zeros_like(batch['latent']), batch['latent']).item()\n",
    "    for i in range(n_iter):\n",
    "        # Compute prediction error\n",
    "        pred = model(batch)\n",
    "        loss = loss_fn(pred['forecast'], batch['y'])\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if clip_val is not None:\n",
    "            for group in optimizer.param_groups:\n",
    "                torch.nn.utils.clip_grad_value_(group[\"params\"], clip_val)\n",
    "\n",
    "        optimizer.step()\n",
    "        # sch = self.lr_schedulers()\n",
    "        # sch.step()\n",
    "\n",
    "        if i % check_int == 0 or i==n_iter-1:\n",
    "            with torch.no_grad():\n",
    "                # recalc without penalties\n",
    "                loss_v = loss_fn(pred['forecast'], batch['y']).item()\n",
    "                if loss_v > diminishing_returns * prev_loss_v and i> 15:\n",
    "                    print(\"Early stopping at optimum\")\n",
    "                    break\n",
    "                prev_loss_v = loss_v\n",
    "                error = loss_fn(model.latent, batch['latent']).item()\n",
    "                if error > diminishing_returns * prev_error and stop_on_truth:\n",
    "                    print(\"Early stopping at minimum prediction error\")\n",
    "                    break\n",
    "                prev_error = error\n",
    "                relerr = sqrt(error/scale)\n",
    "                ##\n",
    "                \n",
    "                big_loss_v = big_loss_fn(pred['forecast'], batch['y']).mean((1,2,3))\n",
    "                print(big_loss_v.shape)\n",
    "                big_error = big_loss_fn(model.latent, batch['latent']).mean((1,2))\n",
    "                big_relerr = torch.sqrt(big_error/scale)\n",
    "                big_losses.append(dict(\n",
    "                    big_loss=big_loss_v.detach().cpu().numpy(),\n",
    "                    big_error = big_error.detach().cpu().numpy(),\n",
    "                    relerr=big_relerr.detach().cpu().numpy()\n",
    "                ))\n",
    "\n",
    "                print(\n",
    "                    f\"loss: {loss:.3e}, error: {error:.3e}, relerror: {relerr:.3e} [{i:>5d}/{n_iter:>5d}]\")\n",
    "                callback(model, i, loss_v, error, loss_fn, batch, pred)\n",
    "\n",
    "    loss_v = loss.item()\n",
    "    error = loss_fn(model.latent, batch['latent']).item()\n",
    "    scale = loss_fn(torch.zeros_like(batch['latent']), batch['latent']).item()\n",
    "    relerr = sqrt(error/scale)\n",
    "    print(\n",
    "        f\"loss: {loss:.3e}, error: {error:.3e}, relerror: {relerr:.3e} scale: {scale:.3e}[{i:>5d}/{n_iter:>5d}]\")\n",
    "\n",
    "    return loss_v, error, relerr, scale, big_losses, big_scale.detach().cpu().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fourier2dMapping(nn.Module):\n",
    "    \"\"\"\n",
    "    Does not work because I tried to do something fancy with parameterizations.\n",
    "    TODO.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, modes: int=20, dims: Tuple[int, int]=(256,256)):\n",
    "        super().__init__()\n",
    "        self.modes = modes  # maybe just normalize the weights?\n",
    "        self.dims = dims\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        map from complex inputs on a half space to real inputs on a full space\n",
    "        \"\"\"\n",
    "        print(\"X\", X.shape, X.dtype)\n",
    "        return fft.irfft2(X, s=self.dims, norm=\"ortho\")\n",
    "\n",
    "    def right_inverse(self, Xp):\n",
    "        \"\"\"\n",
    "        map from real inputs on a full space to complex inputs on a half space\n",
    "        \"\"\"\n",
    "        return fft.rfft2(Xp, s=self.dims, norm=\"ortho\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic version\n",
    "\n",
    "http://pyro.ai/examples/mle_map.html\n",
    "http://pyro.ai/examples/modules.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "887521cc036c2176fc4c7c5fad660bcf5f9a9c2cadfc49851d28bf162a40070d"
  },
  "kernelspec": {
   "display_name": "Cadabra2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
