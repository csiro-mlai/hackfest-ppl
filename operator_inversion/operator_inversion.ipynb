{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference in a PDE model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import numpy as np\n",
    "import random\n",
    "import functools\n",
    "from math import sqrt\n",
    "from PIL import Image\n",
    "\n",
    "from typing import Any, Callable, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('png')\n",
    "# plt.rcParams.update({'figure.figsize': [12, 12]})\n",
    "# plt.rcParams.update({'figure.dpi': 200})\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "%matplotlib inline\n",
    "load_dotenv(find_dotenv());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Any, Dict, Tuple, Optional\n",
    "from math import ceil, sqrt\n",
    "\n",
    "import torch\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "import torch.fft as fft\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from src.nn_modules.fourier_2d_generic import SimpleBlock2dGeneric\n",
    "from src.heatmap import multi_heatmap\n",
    "from src.datamodules.navier_stokes_h5 import NavierStokesH5InstDatastore\n",
    "from src.utils import resolve_path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "# device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inversion by GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Fourier2dMapping(nn.Module):\n",
    "    \"\"\"\n",
    "    Does not work because I tried to do something fancy with parameterizations.\n",
    "    TODO.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, modes: int=20, dims: Tuple[int, int]=(256,256)):\n",
    "        super().__init__()\n",
    "        self.modes = modes  # maybe just normalize the weights?\n",
    "        self.dims = dims\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        map from complex inputs on a half space to real inputs on a full space\n",
    "        \"\"\"\n",
    "        print(\"X\", X.shape, X.dtype)\n",
    "        return fft.irfft2(X, s=self.dims, norm=\"ortho\")\n",
    "\n",
    "    def right_inverse(self, Xp):\n",
    "        \"\"\"\n",
    "        map from real inputs on a full space to complex inputs on a half space\n",
    "        \"\"\"\n",
    "        return fft.rfft2(Xp, s=self.dims, norm=\"ortho\")\n",
    "\n",
    "\n",
    "class NaiveLatent(nn.Module):\n",
    "    def __init__(self,\n",
    "            process_predictor: \"nn.Module\",\n",
    "            dims: Tuple[int, int]=(256,256),\n",
    "            n_batch: int=1):\n",
    "        super().__init__()\n",
    "        self.dims = dims\n",
    "        self.process_predictor = process_predictor\n",
    "        ## Do not fit the process predictor weights\n",
    "        for param in self.process_predictor.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.latent = nn.Parameter(\n",
    "            torch.zeros(\n",
    "                (n_batch, *dims),\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def weights_init(self):\n",
    "        self.latent.data.normal_(0.0, 0.01)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        #copy\n",
    "        batch = dict(**batch)\n",
    "        batch['latent'] = self.latent\n",
    "        return self.process_predictor(batch)\n",
    "\n",
    "\n",
    "\n",
    "def fit(\n",
    "        batch,\n",
    "        model,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        n_iter:int=20,\n",
    "        check_int:int=1,\n",
    "        clip_val: Optional[float] = None,\n",
    "        callback = lambda *x: None,\n",
    "        # pen_0: float = 0.0,\n",
    "        pen_1: float = 0.0,\n",
    "        pen_f: float = 0.0,\n",
    "        stop_on_truth: bool = False,\n",
    "        diminishing_returns=1.1,):\n",
    "    model.train()\n",
    "    model.weights_init()\n",
    "    prev_loss_v = 10^5\n",
    "    prev_error = 10^5\n",
    "    prev_relerr = 10^3\n",
    "    big_losses = []\n",
    "    big_loss_fn = MSELoss(reduction='none')\n",
    "    big_scale = big_loss_fn(torch.zeros_like(batch['latent']), batch['latent']).mean((1,2))\n",
    "    scale = loss_fn(torch.zeros_like(batch['latent']), batch['latent']).item()\n",
    "    for i in range(n_iter):\n",
    "        # Compute prediction error\n",
    "        pred = model(batch)\n",
    "        loss = loss_fn(pred['forecast'], batch['y'])\n",
    "\n",
    "        # if pen_0 > 0.0:\n",
    "            # loss += model.latent.square().mean()* pen_0\n",
    "        if pen_1 > 0.0:\n",
    "            loss += model.latent.diff(dim =-1).square().mean() * pen_1\n",
    "            loss += model.latent.diff(dim =-2).square().mean() * pen_1\n",
    "        if pen_f > 0.0:\n",
    "            loss += (model.latentf.abs().square()).mean() * pen_1\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if clip_val is not None:\n",
    "            for group in optimizer.param_groups:\n",
    "                torch.nn.utils.clip_grad_value_(group[\"params\"], clip_val)\n",
    "\n",
    "        optimizer.step()\n",
    "        # sch = self.lr_schedulers()\n",
    "        # sch.step()\n",
    "\n",
    "        if i % check_int == 0 or i==n_iter-1:\n",
    "            with torch.no_grad():\n",
    "                # recalc without penalties\n",
    "                loss_v = loss_fn(pred['forecast'], batch['y']).item()\n",
    "                if loss_v > diminishing_returns * prev_loss_v and i> 15:\n",
    "                    print(\"Early stopping at optimum\")\n",
    "                    break\n",
    "                prev_loss_v = loss_v\n",
    "                error = loss_fn(model.latent, batch['latent']).item()\n",
    "                if error > diminishing_returns * prev_error and stop_on_truth:\n",
    "                    print(\"Early stopping at minimum prediction error\")\n",
    "                    break\n",
    "                prev_error = error\n",
    "                relerr = sqrt(error/scale)\n",
    "                ##\n",
    "                \n",
    "                big_loss_v = big_loss_fn(pred['forecast'], batch['y']).mean((1,2,3))\n",
    "                print(big_loss_v.shape)\n",
    "                big_error = big_loss_fn(model.latent, batch['latent']).mean((1,2))\n",
    "                big_relerr = torch.sqrt(big_error/scale)\n",
    "                big_losses.append(dict(\n",
    "                    big_loss=big_loss_v.detach().cpu().numpy(),\n",
    "                    big_error = big_error.detach().cpu().numpy(),\n",
    "                    relerr=big_relerr.detach().cpu().numpy()\n",
    "                ))\n",
    "\n",
    "                print(\n",
    "                    f\"loss: {loss:.3e}, error: {error:.3e}, relerror: {relerr:.3e} [{i:>5d}/{n_iter:>5d}]\")\n",
    "                callback(model, i, loss_v, error, loss_fn, batch, pred)\n",
    "\n",
    "    loss_v = loss.item()\n",
    "    error = loss_fn(model.latent, batch['latent']).item()\n",
    "    scale = loss_fn(torch.zeros_like(batch['latent']), batch['latent']).item()\n",
    "    relerr = sqrt(error/scale)\n",
    "    print(\n",
    "        f\"loss: {loss:.3e}, error: {error:.3e}, relerror: {relerr:.3e} scale: {scale:.3e}[{i:>5d}/{n_iter:>5d}]\")\n",
    "\n",
    "    return loss_v, error, relerr, scale, big_losses, big_scale.detach().cpu().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## choosing regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fourierflow.datastores.navier_stokes_h5 import NavierStokesH5InstDatastore\n",
    "torch.manual_seed(60)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "n_batch = 20\n",
    "dims = (256, 256)\n",
    "\n",
    "def plot_heatmap(model, i, loss, error, loss_fn, batch, pred, *args, **kwargs):\n",
    "    target =  batch['latent'][0, :, :].cpu().numpy()\n",
    "    est =  model.latent[0, :, :].cpu().numpy()\n",
    "    err_heatmap = target - est\n",
    "\n",
    "    fig = heatmap.multi_heatmap(\n",
    "        [target, est, err_heatmap],\n",
    "        [\"Target\", \"Estimate\", \"Error\"], *args, **kwargs)\n",
    "    # plt.savefig(f\"paper_ml4ps/inverse_reg_{i}.png\",\n",
    "    #     dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "    # np.savez_compressed(f\"paper_ml4ps/inverse_reg_{i}.npz\",\n",
    "    #     target=target,\n",
    "    #     est=est,\n",
    "    #     error=err_heatmap\n",
    "    # )\n",
    "    plt.show();\n",
    "    plt.close(\"all\");\n",
    "\n",
    "\n",
    "datastore = NavierStokesH5InstDatastore(\n",
    "    '${FNO_DATA_ROOT}/navier-stokes/grf_forcing_mini_1.h5',\n",
    "    n_workers=0,\n",
    "    **{\n",
    "        'ssr': 1,\n",
    "        'n_history': 2,\n",
    "        'n_horizon': 1,\n",
    "        'batch_size': 20,\n",
    "        'latent_key': 'f',\n",
    "        'forcing_key': '',\n",
    "        'param_key': '',\n",
    "    }\n",
    ")\n",
    "dataloader = datastore.val_dataloader()\n",
    "batch = next(iter(dataloader))\n",
    "pp_state_dict = torch.load(\n",
    "        resolve_path('${SM_MODEL_DIR}/history_matching/adequate_checkpoint/fwd-epoch=19-step=26399-valid_loss=0.00000.ckpt'),\n",
    "        map_location=device\n",
    "    )\n",
    "process_predictor = SimpleBlock2dGeneric(\n",
    "    **{\n",
    "        'modes1': 16,\n",
    "        'width': 24,\n",
    "        'n_layers': 4,\n",
    "        'n_history': 2,\n",
    "        'param': False,\n",
    "        'forcing': False,\n",
    "        'latent': True,\n",
    "    }\n",
    ")\n",
    "process_predictor.load_state_dict(\n",
    "    pp_state_dict\n",
    ")\n",
    "model = infer.NaiveLatent(\n",
    "            process_predictor,\n",
    "            dims=dims,\n",
    "            n_batch=n_batch)\n",
    "model.to(device)\n",
    "npbatch = {}\n",
    "for (k,v,) in batch.items():\n",
    "    npbatch[k] = v.cpu().numpy()\n",
    "    batch[k] = v.to(device)\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.005,\n",
    "    weight_decay=0.0)\n",
    "loss_fn = nn.MSELoss().to(device)\n",
    "lambdas = [l**2 for l in range(0, 30)]\n",
    "\n",
    "relerrs = [\n",
    "    infer.fit(\n",
    "        batch,\n",
    "        model,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        n_iter=500,\n",
    "        check_int=5,\n",
    "        clip_val=None,\n",
    "        # callback=plot_heatmap,\n",
    "        # pen_0=pen_0,\n",
    "        pen_1=lambda_,\n",
    "        stop_on_truth=True,\n",
    "    )[2] for lambda_ in lambdas]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(repr(list(zip(lambdas, relerrs))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### longer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_heatmap(model, i, loss, error, loss_fn, batch, pred, *args, **kwargs):\n",
    "    target =  batch['latent'][0, :, :].cpu().numpy()\n",
    "    est =  model.latent[0, :, :].cpu().numpy()\n",
    "    err_heatmap = target - est\n",
    "\n",
    "    fig = heatmap.multi_heatmap(\n",
    "        [target, est, err_heatmap],\n",
    "        [\"Target\", \"Estimate\", \"Error\"], *args, **kwargs)\n",
    "    # plt.savefig(f\"paper_ml4ps/inverse_reg_{i}.png\", dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "    plt.show();\n",
    "    plt.close(\"all\");\n",
    "\n",
    "torch.manual_seed(59)\n",
    "\n",
    "\n",
    "infer.main(\n",
    "    fwd_state_dict_path= '${SM_MODEL_DIR}/history_matching/adequate_long_wide/history_matching/*/checkpoints/fwd-*.ckpt',\n",
    "    fwd_args={\n",
    "        'modes1': 16,\n",
    "        'width': 24,\n",
    "        'n_layers': 4,\n",
    "        'n_history': 10,\n",
    "        'param': False,\n",
    "        'forcing': False,\n",
    "        'latent': True,\n",
    "    },\n",
    "    ds_args={\n",
    "        'ssr': 1,\n",
    "        'n_history': 10,\n",
    "        'n_horizon': 1,\n",
    "        'batch_size': 20,\n",
    "        'latent_key': 'f',\n",
    "        'forcing_key': '',\n",
    "        'param_key': ''\n",
    "    },\n",
    "    callback=plot_heatmap,\n",
    "    lr=0.05, weight_decay=0.00, n_iter=10, pen_1=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas, relerrs = zip(*[(0, 1.6007785766982043), (1, 0.9997765660372001), (4, 0.9007331947795258), (9, 0.8229570081477463), (16, 0.8009129041645531), (25, 0.7878376155161687), (36, 0.7800267568964597), (49, 0.7752918051130337), (64, 0.7707536070233962), (81, 0.7663303353976826), (100, 0.764045919801015), (121, 0.7603965646406099), (144, 0.759816943122361), (169, 0.7553044588047234), (196, 0.7556437640875308), (225, 0.7545405463842202), (256, 0.7592473939734609), (289, 0.769563432528108), (324, 0.7806189493001515), (361, 0.7940838967495987), (400, 0.8070344769974499), (441, 0.8209803543025779), (484, 0.8324510644630306), (529, 0.8433933790498325), (576, 0.8549393999655539), (625, 0.8649656038660587), (676, 0.8746389898996968), (729, 0.8822984327202239), (784, 0.8900433311252656), (841, 0.8969211483359959)])\n",
    "\n",
    "fig = plt.plot(lambdas, relerrs, )\n",
    "plt.xlabel(r'$\\lambda$')\n",
    "plt.ylabel(\"relative error\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(0.75, 1)\n",
    "plt.savefig(f\"paper_ml4ps/inverse_reg_lambda.png\", dpi=300, bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = model.latent.cpu().detach().numpy()\n",
    "\n",
    "multi_heatmap([arr[0], arr[1], arr[2]], [\"1\", \"2\", \"3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer.fit(\n",
    "    batch,\n",
    "    model,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    n_iter=1000,\n",
    "    check_int=5,\n",
    "    clip_val=None,\n",
    "    # callback=plot_heatmap,\n",
    "    # pen_0=pen_0,\n",
    "    pen_1=30,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = model.latent.cpu().detach().numpy()\n",
    "for i in range(5):\n",
    "    est = arr[i]\n",
    "    target = npbatch['latent'][i]\n",
    "    err = est-target\n",
    "    multi_heatmap([est, target, err], [\"est\", \"target\", \"Error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = NavierStokesH5InstDatastore(\n",
    "    '${FNO_DATA_ROOT}/navier-stokes/grf_forcing_mini.h5',\n",
    "    **{\n",
    "        'ssr': 1,\n",
    "        'n_history': 2,\n",
    "        'n_horizon': 1,\n",
    "        'batch_size': 20,\n",
    "        'latent_key': 'f',\n",
    "        'forcing_key': '',\n",
    "        'param_key': ''\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### longer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_heatmap(model, i, loss, error, loss_fn, batch, pred, *args, **kwargs):\n",
    "    target =  batch['latent'][0, :, :].cpu().numpy()\n",
    "    est =  model.latent[0, :, :].cpu().numpy()\n",
    "    err_heatmap = target - est\n",
    "\n",
    "    fig = heatmap.multi_heatmap(\n",
    "        [target, est, err_heatmap],\n",
    "        [\"Target\", \"Estimate\", \"Error\"], *args, **kwargs)\n",
    "    # plt.savefig(f\"paper_ml4ps/inverse_reg_{i}.png\", dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "    plt.show();\n",
    "    plt.close(\"all\");\n",
    "\n",
    "torch.manual_seed(59)\n",
    "\n",
    "\n",
    "infer.main(\n",
    "    fwd_state_dict_path= '${SM_MODEL_DIR}/history_matching/adequate_long_wide/history_matching/*/checkpoints/fwd-*.ckpt',\n",
    "    fwd_args={\n",
    "        'modes1': 16,\n",
    "        'width': 24,\n",
    "        'n_layers': 4,\n",
    "        'n_history': 10,\n",
    "        'param': False,\n",
    "        'forcing': False,\n",
    "        'latent': True,\n",
    "    },\n",
    "    ds_args={\n",
    "        'ssr': 1,\n",
    "        'n_history': 10,\n",
    "        'n_horizon': 1,\n",
    "        'batch_size': 20,\n",
    "        'latent_key': 'f',\n",
    "        'forcing_key': '',\n",
    "        'param_key': ''\n",
    "    },\n",
    "    callback=plot_heatmap,\n",
    "    lr=0.05, weight_decay=0.00, n_iter=10, pen_1=30)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "887521cc036c2176fc4c7c5fad660bcf5f9a9c2cadfc49851d28bf162a40070d"
  },
  "kernelspec": {
   "display_name": "Cadabra2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
