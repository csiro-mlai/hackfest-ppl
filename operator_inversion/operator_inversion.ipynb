{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference in a neural emulation model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look, we can do approximate probabilistic inference with a complicated neural network!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt, ceil, floor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.fft as fft\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.modules.loss import MSELoss\n",
    "from torch.functional import F \n",
    "import einops\n",
    "\n",
    "\n",
    "from src.nn_modules.fourier_2d_generic import SimpleBlock2dGeneric\n",
    "from src.heatmap import multi_heatmap\n",
    "from src.utils import resolve_path\n",
    "\n",
    "# device = torch.device('cuda')  # I can't make MCMC work on the GPU.\n",
    "device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is packed as as $(b, x, y, t)$ i.e. batch first, coordinates in the middle, time last. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./data/grf_forcing_single.npz')\n",
    "\n",
    "def get_obs(data, t=0, n_steps=2, y=False):\n",
    "    x = data['u'][..., t:t+n_steps]\n",
    "    latent = data['f'][...]\n",
    "    obs= {\n",
    "        'x': x,\n",
    "        'latent': latent\n",
    "    }\n",
    "    if y:\n",
    "        obs['y'] = data['u'][..., t+n_steps]\n",
    "\n",
    "    return obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_as_tensor(d, device=device):\n",
    "    \"\"\"\n",
    "    it was faster to write this function than to search the docs for it\n",
    "    \"\"\"\n",
    "    return {k: torch.as_tensor(v).to(device) for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def multi_img_plot_time(x, batch=0, interval=1, n_cols=None, fsize=6):\n",
    "    \"\"\"\n",
    "    Plot multiple timesteps of an array (time last)\n",
    "    \"\"\"\n",
    "    steps = range(0, x.shape[-1], interval)\n",
    "    if n_cols is None:\n",
    "        n_cols = len(steps)\n",
    "    print(steps, len(steps), n_cols)\n",
    "    n_rows = ceil(len(steps) / n_cols)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(fsize * n_cols/n_rows, fsize));\n",
    "    axes = axes.flatten()\n",
    "    for ax in axes:\n",
    "        ax.set_axis_off()\n",
    "    for i, ax in zip(steps, axes):\n",
    "        ax.imshow(x[batch,..., i])\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def multi_img_plot_batch(x, interval=1, n_cols=None, fsize=6):\n",
    "    \"\"\"\n",
    "    Plot multiple batches of an array \n",
    "    \"\"\"\n",
    "    if len(x.shape) == 4:\n",
    "        x = np.squeeze(x, -1)\n",
    "\n",
    "    steps = range(0, x.shape[0], interval)\n",
    "    if n_cols is None:\n",
    "        n_cols = len(steps)\n",
    "    n_rows = ceil(len(steps) / n_cols)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(fsize * n_cols/n_rows, fsize));\n",
    "    axes = axes.flatten()\n",
    "    for ax in axes:\n",
    "        ax.set_axis_off()\n",
    "    for i, ax in zip(steps, axes):\n",
    "        ax.imshow(x[..., i])\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def img_plot(x, interval=1, n_cols=None, fsize=6):\n",
    "    \"\"\"\n",
    "    plot whatever image I can find\n",
    "    \"\"\"\n",
    "    x = np.squeeze(x, )\n",
    "    plt.imshow(x)\n",
    "    plt.gca().set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "multi_img_plot_time(get_obs(data, 0, -1)['x'], n_cols=4, interval=5);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_plot(get_obs(data, 0, -1)['latent']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process predictor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_state_dict = torch.load(\n",
    "    './models/fno_forward.ckpt',\n",
    "    map_location=device\n",
    ")\n",
    "process_predictor = SimpleBlock2dGeneric(\n",
    "    modes1=16,\n",
    "    width=24,\n",
    "    n_layers=4,\n",
    "    n_history=2,\n",
    "    param=False,\n",
    "    forcing=False,\n",
    "    latent=True,\n",
    ")\n",
    "process_predictor.load_state_dict(\n",
    "    pp_state_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_forward(x, latent, n_horizon=1, n_steps=2):\n",
    "    x = torch.as_tensor(x).to(device)\n",
    "    latent = torch.as_tensor(latent).to(device)\n",
    "    process_predictor.to(device)\n",
    "    for i in range(n_horizon):\n",
    "        pred = process_predictor({'x': x[...,-(n_steps):], 'latent': latent})['forecast']\n",
    "        x = torch.cat((x, pred), dim=-1)\n",
    "    return x[...,-n_horizon:]\n",
    "\n",
    "obs = get_obs(data, 0, 10)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = predict_forward(obs['x'], obs['latent'], n_horizon=50, n_steps=2)\n",
    "\n",
    "multi_img_plot_time(pred.cpu().numpy(), n_cols=5, interval=5);\n",
    "plt.savefig('./fno_forward_predict_sheet.jpg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inversion by GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RasterLatent(nn.Module):\n",
    "    def __init__(self,\n",
    "            process_predictor: \"nn.Module\",\n",
    "            dims = (256,256),\n",
    "            latent_dims = None,\n",
    "            interpolation = 'bilinear',\n",
    "            n_batch: int=1):\n",
    "        super().__init__()\n",
    "        self.dims = dims\n",
    "        if latent_dims is None:\n",
    "            latent_dims = dims\n",
    "        self.latent_dims = latent_dims\n",
    "        self.interpolation = interpolation\n",
    "        self.process_predictor = process_predictor\n",
    "        ## Do not fit the process predictor weights\n",
    "        process_predictor.train(False)\n",
    "        for param in self.process_predictor.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.latent = nn.Parameter(\n",
    "            torch.zeros(\n",
    "                (n_batch, *latent_dims),\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def get_latent(self):\n",
    "        if self.latent_dims==self.dims:\n",
    "            return self.latent\n",
    "        return F.interpolate(\n",
    "            self.latent.unsqueeze(1),\n",
    "            self.dims,\n",
    "            mode=self.interpolation\n",
    "        ).squeeze(1)  #squeeze is because of weird interpolate semantics\n",
    "\n",
    "    def weights_init(self, scale=0.005):\n",
    "        self.latent.data.normal_(0.0, scale)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        batch = dict(**batch)\n",
    "        batch['latent'] = self.get_latent()\n",
    "        return self.process_predictor(batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicLatent(nn.Module):\n",
    "    def __init__(self,\n",
    "            process_predictor: \"nn.Module\",\n",
    "            dims = (256,256),\n",
    "            modes = 17,\n",
    "            n_batch: int=1):\n",
    "        super().__init__()\n",
    "        self.dims = dims\n",
    "        self.modes = modes\n",
    "        self.process_predictor = process_predictor\n",
    "        ## Do not fit the process predictor weights\n",
    "        process_predictor.train(False)\n",
    "        for param in self.process_predictor.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.latent_spect = nn.Parameter(\n",
    "            torch.zeros(\n",
    "                (n_batch, modes*2-1, modes, 2),\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_latent(self):\n",
    "        cplx_latent = torch.view_as_complex(self.latent_spect)\n",
    "        ft_padded = F.pad(cplx_latent, (\n",
    "            0,  # left\n",
    "            self.dims[1]//2-self.modes, # right\n",
    "            0, # top\n",
    "            (self.dims[0]+1-self.modes), # bottom\n",
    "        ), \"constant\", 0)\n",
    "        \n",
    "        #Return to physical space\n",
    "        return torch.fft.irfft2(ft_padded, s=self.dims, norm=\"backward\")\n",
    "\n",
    "    def weights_init(self, scale=1.0):\n",
    "        self.latent_spect.data.normal_(0.0, scale)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        batch = dict(**batch)\n",
    "        batch['latent'] = self.get_latent()\n",
    "        return self.process_predictor(batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "        batch,\n",
    "        model,\n",
    "        optimizer,\n",
    "        n_iter: int=20,\n",
    "        check_int: int=1,\n",
    "        clip_val = None,\n",
    "        init_scale=0.1):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    model.weights_init(init_scale)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    big_loss_fn = nn.MSELoss(reduction='none')\n",
    "    scale = loss_fn(torch.zeros_like(batch['latent']), batch['latent']).item()\n",
    "    for i in range(n_iter):\n",
    "        # Compute prediction error\n",
    "        pred = model(batch)\n",
    "        loss = loss_fn(pred['forecast'], batch['y'])\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if clip_val is not None:\n",
    "            for group in optimizer.param_groups:\n",
    "                torch.nn.utils.clip_grad_value_(group[\"params\"], clip_val)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % check_int == 0 or i==n_iter-1:\n",
    "            with torch.no_grad():\n",
    "                loss_v = loss.item()\n",
    "                # batchwise error\n",
    "                big_error = big_loss_fn(model.get_latent(), batch['latent']).mean(dim=(1,2))\n",
    "                big_relerr = torch.sqrt(big_error/scale)\n",
    "                error = big_error.mean().item()\n",
    "                relerr = sqrt(big_relerr.mean().item())\n",
    "                print(\n",
    "                    f\"loss: {loss:.3e}, error: {error:.3e}, relerror: {relerr:.3e} [{i:>5d}/{n_iter:>5d}]\")\n",
    "\n",
    "                target =  batch['latent'][0, :, :].cpu().numpy()\n",
    "                est =  model.get_latent()[0, :, :].cpu().numpy()\n",
    "                err_heatmap = target - est\n",
    "                multi_heatmap(\n",
    "                    [target, est, err_heatmap],\n",
    "                    [\"Target\", \"Estimate\", \"Error\"])\n",
    "                plt.show();\n",
    "                plt.close(\"all\");\n",
    "\n",
    "    return loss_v, error, relerr, scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.latent.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RasterLatent(\n",
    "    process_predictor,\n",
    "    dims=obs['x'].shape[1:3],\n",
    "    latent_dims=(16,16),\n",
    "    n_batch=1)\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0025,\n",
    "    weight_decay=0.0)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "fit(\n",
    "    dict_as_tensor(get_obs(data,t=10,n_steps=2,y=True)),\n",
    "    model,\n",
    "    optimizer,\n",
    "    n_iter=50,\n",
    "    check_int=10,\n",
    "    clip_val=None,\n",
    "    init_scale=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CyclicLatent(\n",
    "    process_predictor,\n",
    "    dims=obs['x'].shape[1:3],\n",
    "    n_batch=1)\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.25,\n",
    "    weight_decay=0.01)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "fit(\n",
    "    dict_as_tensor(get_obs(data,t=10,n_steps=2,y=True)),\n",
    "    model,\n",
    "    optimizer,\n",
    "    n_iter=250,\n",
    "    check_int=10,\n",
    "    clip_val=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RasterLatent(\n",
    "    process_predictor,\n",
    "    dims=obs['x'].shape[1:3],\n",
    "    n_batch=1)\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.00025,\n",
    "    weight_decay=10)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "fit(\n",
    "    dict_as_tensor(get_obs(data,t=0,n_steps=2,y=True)),\n",
    "    model,\n",
    "    optimizer,\n",
    "    n_iter=50,\n",
    "    check_int=10,\n",
    "    clip_val=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic version\n",
    "\n",
    "* [Modules in Pyro — Pyro Tutorials](https://pyro.ai/examples/modules.html)\n",
    "* [Neural Networks — Pyro documentation](https://docs.pyro.ai/en/stable/nn.html#pyro.nn.module.PyroSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "from pyro.nn import PyroModule, PyroParam, PyroSample\n",
    "from pyro.nn.module import to_pyro_module_\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO, Predictive, MCMC, NUTS\n",
    "from pyro import poutine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbRasterLatent(PyroModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            process_predictor: \"nn.Module\",\n",
    "            dims = (256,256),\n",
    "            prior_scale = 0.01,\n",
    "            obs_scale = 0.01,):\n",
    "        super().__init__()\n",
    "        self.dims = dims\n",
    "        self.prior_scale = prior_scale\n",
    "        self.obs_scale = obs_scale\n",
    "        self.process_predictor = process_predictor\n",
    "        process_predictor.train(False)\n",
    "        ## Do not fit the process predictor weights\n",
    "        for param in self.process_predictor.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.latent = PyroSample(dist.Normal(0, 0.01).expand(dims).to_event(2))\n",
    "\n",
    "    def get_latent(self):\n",
    "        return self.latent\n",
    "\n",
    "    def forward(self, X, y=None):\n",
    "        #overwrite process predictor batch with my own latent\n",
    "        mean = self.process_predictor({\n",
    "            'x': X,\n",
    "            'latent': self.get_latent().unsqueeze(0).to(device) #<- why needed?,\n",
    "        })['forecast']\n",
    "        return pyro.sample(\n",
    "            \"obs\", dist.Normal(mean, self.obs_scale).to_event(2),\n",
    "            obs=y)\n",
    "\n",
    "obs = dict_as_tensor(get_obs(data, 0, 2, y=True))\n",
    "\n",
    "model = ProbRasterLatent(\n",
    "    process_predictor.to(device),\n",
    "    dims=obs['x'].shape[1:3],\n",
    "    prior_scale=0.01,\n",
    "    obs_scale=0.01,\n",
    ")\n",
    "model.to(device)\n",
    "nuts_kernel = NUTS(model, full_mass=False, max_tree_depth=5, jit_compile=True) # high performacne config\n",
    "\n",
    "mcmc = MCMC(nuts_kernel, num_samples=100, warmup_steps=100, num_chains=1)\n",
    "mcmc.run(obs['x'], obs['y'])\n",
    "mc_samples = {k: v.detach().cpu().numpy() for k, v in mcmc.get_samples().items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbCyclicLatent(nn.Module):\n",
    "    def __init__(self,\n",
    "            process_predictor: \"nn.Module\",\n",
    "            dims = (256,256),\n",
    "            modes = 17,\n",
    "            n_batch: int=1,\n",
    "            prior_scale=10.0,\n",
    "            obs_scale = 0.01):\n",
    "        super().__init__()\n",
    "        self.dims = dims\n",
    "        self.modes = modes\n",
    "        self.process_predictor = process_predictor\n",
    "        ## Do not fit the process predictor weights\n",
    "        process_predictor.train(False)\n",
    "        for param in self.process_predictor.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.obs_scale = obs_scale\n",
    "        self.latent_spect = PyroSample(\n",
    "            dist.Normal(0, prior_scale).expand(\n",
    "                (n_batch, modes*2-1, modes, 2)\n",
    "            ).to_event(2))\n",
    "\n",
    "    def get_latent(self):\n",
    "        ## complex values are flakey in pyro\n",
    "        cplx_latent = self.latent_spect[...,0] + 1j*self.latent_spect[...,1]\n",
    "        ft_padded = F.pad(cplx_latent, (\n",
    "            0,  # left\n",
    "            self.dims[1]//2-self.modes, # right\n",
    "            0, # top\n",
    "            (self.dims[0]+1-self.modes), # bottom\n",
    "        ), \"constant\", 0)\n",
    "        #\n",
    "        #Return to physical space\n",
    "        return torch.fft.irfft2(ft_padded, s=self.dims, norm=\"backward\")\n",
    "        \n",
    "    def forward(self, X, y=None):\n",
    "        #overwrite process predictor batch with my own latent\n",
    "        mean = self.process_predictor({\n",
    "            'x': X,\n",
    "            'latent': self.get_latent().unsqueeze(0),\n",
    "        })['forecast']\n",
    "        pred =  pyro.sample(\n",
    "            \"obs\", dist.Normal(mean, self.obs_scale).to_event(2),\n",
    "            obs=y)\n",
    "        print('pred', pred.shape)\n",
    "        return pred\n",
    "\n",
    "model = ProbCyclicLatent(\n",
    "    process_predictor,\n",
    "    dims=obs['x'].shape[1:3],\n",
    "    prior_scale=10.0,\n",
    "    obs_scale=0.01,\n",
    ")\n",
    "\n",
    "nuts_kernel = NUTS(model, full_mass=False, max_tree_depth=5, jit_compile=True) # high performacne config\n",
    "\n",
    "mcmc = MCMC(nuts_kernel, num_samples=100, warmup_steps=100)\n",
    "obs = get_obs(data, 0, 2, y=True)\n",
    "mcmc.run(torch.as_tensor(obs['x']), torch.as_tensor(obs['y']))\n",
    "mc_samples = {k: v.detach().cpu().numpy() for k, v in mcmc.get_samples().items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.latent_spect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"data/samples1.npz\", mc_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_img_batch(mc_samples['latent'], interval=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## offcuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CyclicLatent(\n",
    "    process_predictor,\n",
    "    dims=obs['x'].shape[1:3],\n",
    "    n_batch=1)\n",
    "model.latent_spect.data[0,5,6,0] = 2\n",
    "model.latent_spect.data[0,5,5,0] = 1\n",
    "model.latent_spect.data[0,5,5,1] = 1\n",
    "field = model.get_latent().cpu().detach().numpy()\n",
    "field.shape\n",
    "img_plot(field);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclic_basis(\n",
    "        dim_sizes,\n",
    "        device=None):\n",
    "    def generate_grid(size):\n",
    "        return torch.linspace(\n",
    "            pos_low, pos_high, steps=size,\n",
    "            device=device)\n",
    "    grid_list = list(map(generate_grid, dim_sizes))\n",
    "    pos = torch.stack(torch.meshgrid(*grid_list), dim=-1)\n",
    "    # pos.shape == [*dim_sizes, n_dims]\n",
    "    return pos\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "887521cc036c2176fc4c7c5fad660bcf5f9a9c2cadfc49851d28bf162a40070d"
  },
  "kernelspec": {
   "display_name": "Cadabra2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
