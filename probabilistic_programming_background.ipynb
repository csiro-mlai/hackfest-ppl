{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is probabilistic programming? And why?\n",
    "\n",
    "A probabilistic programming system is a system for specifying stochastic\n",
    "generative models, and reasoning about them. Or, as\n",
    "[Fabiana Clemente puts it](https://towardsdatascience.com/intro-to-probabilistic-programming-b47c4e926ec5)\n",
    "\n",
    "> Probabilistic programming is about doing statistics using the tools of\n",
    "> computer science.\n",
    "\n",
    "The kinds of problem that we attempt to solve with probabilistic programming are\n",
    "ones where the inputs and outputs of programs are not just numbers, but whole\n",
    "probability distributions the space of possibly values of those numbers. Here,\n",
    "probability distributions are first class citizens.\n",
    "\n",
    "Conceptually, probabilistic programming is a way of automating operations in\n",
    "[probabilistic graphical models](https://en.wikipedia.org/wiki/Graphical_model),\n",
    "similarly to how automatic differentiation frameworks (like Pytorch or\n",
    "Tensorflow) can automate the operations of forward and backward propagation. a\n",
    "PPL provides tools that make it easy to build a programmatic representation of\n",
    "your probabilistic model, even if you've never thought of it specifically in\n",
    "terms of a *graphical* model (e.g. if you only have your model as a set of\n",
    "equations). Once that's accomplished, you can use the PPL's built-in functions\n",
    "to perform typical operations on the model, such as sampling, conditioning,\n",
    "computing probabilities, etc. This means that you can do verious nifty things,\n",
    "like Bayesian inference, uncertainty quantification, experiment design etc.\n",
    "\n",
    "One important benefit of modern toolkits is that you (usually) get\n",
    "backpropagation of gradients for free. This is extremely useful if you want to\n",
    "apply gradient-based methods for inference, especially if your model is complex\n",
    "and uses more obscure distributions than a mixture of Gaussians. We will see\n",
    "over the course of the hackfest just how quick and easy it is to turn an\n",
    "arbitrary set of equations into a working probabilistic model that you can then\n",
    "run ML algorithms on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why `pyro`?\n",
    "\n",
    "There are [too many PPLs](https://en.wikipedia.org/wiki/Probabilistic_programming)!\n",
    "But pyro is the one that is probably closest to gaining critical mass.\n",
    "It is not the simplest one, but \n",
    "\n",
    "1. it _does_ integrate high quality versions of hip neural network methods into the\n",
    "   classic PPL methods. This is typically a good predictor of a framework's\n",
    "   longevity, so hopefully the skills you develop in this hackfest won't become\n",
    "   obsolete too quickly.\n",
    "2. It is built of pytorch, which many of us here already use.\n",
    "3. Finally, we have more experience inside the FSP with pyro than with \n",
    "   any other PPL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Other PPLs\n",
    "\n",
    "* [Stan](https://mc-stan.org/) (R/Python/CLI/…)\n",
    "* [Turing.jl](https://turing.ml/stable/) (julia)\n",
    "* [Gen](https://www.gen.dev/) (julia)\n",
    "* [NumPyro documentation](http://num.pyro.ai/en/stable/) (jax)\n",
    "* [Edward2](https://github.com/google/edward2) (Tensorflow/Jax)\n",
    "* Why not write your own? Everyone else seems to.\n",
    "\n",
    "\n",
    "## Additional Reading\n",
    "\n",
    "* Jan-Willem van de Meent, Brooks Paige, Hongseok Yang, Frank Wood, [An Introduction to Probabilistic Programming](https://arxiv.org/abs/1809.10756)\n",
    "* [Bayes for Hackers](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers)\n",
    "* [Rob Salomone’s course](https://robsalomone.com/course-deep-probabilistic-models/) is great for showing off some super modern techniques, like flows and amortized inference.\n",
    "* [Statistical Rethinking | Richard McElreath](https://xcelab.net/rm/statistical-rethinking/) has gone viral as an introduction to some of this stuff.\n",
    "  It is [available on O’Reilly](https://learning.oreilly.com/library/view/statistical-rethinking-2nd/9780429639142/) (free for CSIRO people).\n",
    "  There is a \n",
    "  [PyMC3](https://github.com/gbosquechacon/statrethink_course_in_pymc3)\n",
    "  [and a numpyro](https://github.com/asuagar/statrethink-course-in-numpyro/) version.\n",
    "* Kevin Murphy, [Probabilistic Machine Learning: An Introduction](https://probml.github.io/pml-book/book1.html)\n",
    "* Wi Ji Ma, Konrad Paul Kording, Daniel Goldreich, [Bayesian models of perception and action](https://www.bayesianmodeling.com)\n",
    "* Noah D. Goodman, Joshua B. Tenenbaum et al, [Probabilistic Models of Cognition - 2nd Edition](http://probmods.org/)\n",
    "* Noah D. Goodman and Andreas Stuhlmüller, [The Design and Implementation of Probabilistic Programming Languages](http://dippl.org/)\n",
    "* [Bayesian statistics for _Nature_ readers](https://www.nature.com/articles/s43586-020-00001-2)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "887521cc036c2176fc4c7c5fad660bcf5f9a9c2cadfc49851d28bf162a40070d"
  },
  "keep_output": true,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc-showcode": true,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
