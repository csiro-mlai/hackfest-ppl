{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we demonstrate how pyro can be used to do inference on complex\n",
    "probabilistic models. We will examine a problem of locating signal sources in\n",
    "2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim\n",
    "import seaborn\n",
    "import torch\n",
    "\n",
    "from contextlib import ExitStack\n",
    "from eig import elbo_learn\n",
    "from pyro.contrib.util import iter_plates_to_shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We have two signal sources and the aim is to infer their locations\n",
    "$$\\theta = {x_1, y_1, x_2, y_2}$$.\n",
    "Our data is noisy samples of signal strength taken at two-dimensional\n",
    "coordinates $\\xi$. The individual signal strengths follow an inverse square law\n",
    "and the total intensity at $\\xi$ is the superposition of individual signals:\n",
    "\n",
    "$$\\mu(\\theta, \\xi) = b + \\frac{1}{m + ||\\theta_1 - \\xi||} + \\frac{1}{m + ||\\theta_2 - \\xi||},$$\n",
    "\n",
    "We can plot the signal map for a given instantiation of $\\theta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "b, m = 1e-1, 1e-4\n",
    "\n",
    "def get_signal(xi, s1, s2):\n",
    "    d1 = np.square(xi-s1).sum(axis=-1)\n",
    "    d2 = np.square(xi-s2).sum(axis=-1)\n",
    "    return np.log(b + 1 / (m + d1) + 1 / (m + d2))\n",
    "\n",
    "x = np.arange(-4,4,0.01)\n",
    "y = np.arange(-4,4,0.01)\n",
    "xy = np.transpose([np.tile(x, len(y)), np.repeat(y, len(x))])\n",
    "s1 = np.array([-0.2963,  2.6764])\n",
    "s2 = np.array([-0.1408, -0.8441])\n",
    "log_mu = get_signal(xy, s1, s2)\n",
    "z = log_mu.reshape(800,800)\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "cs = plt.contourf(x,y, z, 64, cmap=cm.Blues)\n",
    "plt.grid(True); plt.yticks(fontsize=20); plt.xticks(fontsize=20);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "where `b` is the background signal and `m` the maximum signal, respectively.\n",
    "Our prior belief over the location of the sources is a standard normal:\n",
    "\n",
    "$\\theta_k \\sim \\mathcal{N}(0, \\mathcal{I})$.\n",
    "\n",
    "Signal measurements have Gaussian noise $\\sigma$, and we consider the\n",
    "log-strength for convenience:\n",
    "\n",
    "$\\log y | \\theta, \\xi \\sim \\mathcal{N} (\\log \\mu(\\theta, \\xi), \\sigma)$\n",
    "\n",
    "Lets create our model and guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_source_model(theta_mu, theta_sig, observation_sd, alpha=1,\n",
    "                      observation_label=\"y\", b=1e-1, m=1e-4):\n",
    "    def source_model(design):\n",
    "        batch_shape = design.shape[:-2]\n",
    "        with ExitStack() as stack:\n",
    "            for plate in iter_plates_to_shape(batch_shape):\n",
    "                stack.enter_context(plate)\n",
    "            theta_shape = batch_shape + theta_mu.shape[-2:]\n",
    "            theta = pyro.sample(\n",
    "                \"theta\",\n",
    "                dist.Normal(\n",
    "                    theta_mu.expand(theta_shape),\n",
    "                    theta_sig.expand(theta_shape)\n",
    "                ).to_event(2)\n",
    "            )\n",
    "            distance = torch.square(\n",
    "                design.unsqueeze(-2) - theta.unsqueeze(-3)\n",
    "            ).sum(dim=-1)\n",
    "            ratio = alpha / (m + distance)\n",
    "            mu = b + ratio.sum(dim=-1)\n",
    "            emission_dist = dist.Normal(\n",
    "                torch.log(mu), observation_sd\n",
    "            ).to_event(1)\n",
    "            y = pyro.sample(observation_label, emission_dist)\n",
    "            return y\n",
    "\n",
    "    return source_model\n",
    "\n",
    "def elboguide(design, dim=1):\n",
    "    theta_mu = pyro.param(\"theta_mu\", torch.zeros(dim, 1, 2, 2))\n",
    "    theta_sig = pyro.param(\"theta_sig\", torch.ones(dim, 1, 2, 2),\n",
    "                           constraint=torch.distributions.constraints.positive)\n",
    "    batch_shape = design.shape[:-2]\n",
    "    with ExitStack() as stack:\n",
    "        for plate in iter_plates_to_shape(batch_shape):\n",
    "            stack.enter_context(plate)\n",
    "        theta_shape = batch_shape + theta_mu.shape[-2:]\n",
    "        pyro.sample(\"theta\", dist.Normal(\n",
    "            theta_mu.expand(theta_shape),\n",
    "            theta_sig.expand(theta_shape)).to_event(2)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And lets generate our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_srcs = 2\n",
    "n_dims = 2\n",
    "obs_sd = 0.5\n",
    "theta_mu = torch.zeros(1, 1, n_srcs, n_dims)\n",
    "theta_sig = torch.ones(1, 1, n_srcs, n_dims)\n",
    "true_theta = torch.distributions.Normal(theta_mu, theta_sig).sample()\n",
    "true_model = pyro.condition(\n",
    "    make_source_model(theta_mu, theta_sig, obs_sd),\n",
    "    {\"theta\": true_theta}\n",
    ")\n",
    "s1 = true_theta[...,0].squeeze().cpu().numpy()\n",
    "s2 = true_theta[...,1].squeeze().cpu().numpy()\n",
    "print(f\"True \\u03B8:\\n\\u03B8_1 = {s1}\\n\\u03B8_2 = {s2}\")\n",
    "\n",
    "xi_x = np.arange(-4,4,0.4)\n",
    "xi_y = np.arange(-4,4,0.4)\n",
    "xi = np.transpose([np.tile(xi_x, len(xi_y)), np.repeat(xi_y, len(xi_x))])\n",
    "xi = torch.tensor(xi).unsqueeze(0).unsqueeze(0)\n",
    "ys = true_model(xi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we do a bit of magic by calling on a helper function that abstracts away the\n",
    "SVI loop. What's important to understand here is the interface expose by\n",
    "`elbo_learn`. As arguments, we pass:\n",
    "\n",
    "&nbsp; &nbsp; 1. The prior\n",
    "\n",
    "&nbsp; &nbsp; 2. The training inputs $\\xi$\n",
    "\n",
    "&nbsp; &nbsp; 3. The observation site name\n",
    "\n",
    "&nbsp; &nbsp; 4. the parameter site name\n",
    "\n",
    "&nbsp; &nbsp; 5-6. The number of samples and steps for gradient descent\n",
    "\n",
    "&nbsp; &nbsp; 7. The guide\n",
    "\n",
    "&nbsp; &nbsp; 8. A dictionary of training targets\n",
    "\n",
    "&nbsp; &nbsp; 9. An SGD optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prior = make_source_model(\n",
    "    torch.zeros(1, 1, n_srcs, n_dims),\n",
    "    torch.ones(1, 1, n_srcs, n_dims),\n",
    "    obs_sd\n",
    ")\n",
    "\n",
    "elbo_n_samples, elbo_n_steps, elbo_lr = 100, 1000, 0.04\n",
    "loss = elbo_learn(\n",
    "    prior, xi, [\"y\"], [\"theta\"], elbo_n_samples, elbo_n_steps,\n",
    "    elboguide, {\"y\": ys}, optim.Adam({\"lr\": elbo_lr})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "`elbo_learn` minimises the ELBO loss w.r.t. the parameters of the guide,\n",
    "`theta_mu` and `theta_sig`. We can now extract them from the pyro param store\n",
    "and plot the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "theta_mu = pyro.param(\"theta_mu\").detach().data.clone().squeeze()\n",
    "theta_sig = pyro.param(\"theta_sig\").detach().data.clone().squeeze()\n",
    "posterior0 = dist.Normal(theta_mu[0], theta_sig[0])\n",
    "posterior1 = dist.Normal(theta_mu[1], theta_sig[1])\n",
    "n_samples = 10000\n",
    "samples0 = posterior0.sample((n_samples,)).cpu().numpy()\n",
    "samples1 = posterior1.sample((n_samples,)).cpu().numpy()\n",
    "samples = np.concatenate([samples0, samples1])\n",
    "hue = np.concatenate([np.zeros(n_samples), np.ones(n_samples)])\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "seaborn.kdeplot(x=samples[...,0], y=samples[...,1], fill=True, hue=hue,\n",
    "                legend=False, thresh=0.01)\n",
    "np_theta = true_theta.squeeze().cpu().numpy()\n",
    "plt.scatter(np_theta[:,0], np_theta[:,1], color=\"green\", marker=\"x\", s=100)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
