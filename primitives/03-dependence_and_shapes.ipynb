{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Pyro distributions accept and return Pytorch tensors, which have a `shape`.\n",
    "However, when dealing with the distributions themselves, there are several\n",
    "`shape`s to keep in mind: `event_shape`, `batch_shape`, and `sample_shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "\n",
    "from pyro import poutine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`event_shape` can be thought of as the shape of a single event or draw from the\n",
    "distribution. The general rule is that a distribution over tensors of order `n`\n",
    "have an `event_shape` that is an n-tuple. A univariate distribution\n",
    "(e.g. `Normal`) is distributed over a single variable, which can be considered\n",
    "a tensor of order `0`. Hence, such distributions have an empty `event_shape`.\n",
    "Multivariate distributions over vectors of variables (e.g. `MultivariateNormal`)\n",
    "have an `event_shape` that is a 1-tuple, since vectors are tensors of order `1`.\n",
    "Distributions over matrices of variables (e.g. `LKJ`) have 2-tuple\n",
    "`event_shape`s, and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(dist.Normal(torch.zeros(5), torch.ones(5)).event_shape)\n",
    "print(dist.MultivariateNormal(torch.zeros(5), torch.eye(5)).event_shape)\n",
    "print(dist.LKJ(5).event_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may seem trivial when you're only drawing a single sample, but when you're\n",
    "drawing multiple batches of samples in parallel (which is easy with pyro), the\n",
    "`event_shape` has a special meaning: it represents the shape of the dependent\n",
    "random variables in the draw. If we sample from a multivariate distribution,\n",
    "variables that are part of the same event (i.e. samples where the index differs\n",
    "only in the rightmost `len(event_shape)` dimensions) are dependent on each\n",
    "other, whereas variables belonging to different events (i.e. samples where the\n",
    "index differs outside the rightmost `len(event_shape)` dimensions) are\n",
    "independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cov = torch.rand(5,5).abs()\n",
    "cov = torch.mm(cov, cov.t()) + 0.01 * torch.eye(5)\n",
    "d = dist.MultivariateNormal(torch.zeros(2,5), torch.tile(cov, (2,1,1)))\n",
    "print(f\"event shape: {d.event_shape}\")\n",
    "s = d.sample((100000,))\n",
    "cond_s = s[s[:,0,0] > 1.]\n",
    "plt.hist(s[:,0,1].cpu().numpy(), bins=100, density=True, color=\"red\", alpha=0.5)\n",
    "plt.hist(cond_s[:,0,1].cpu().numpy(), bins=100, density=True, color=\"blue\", alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(s[:,1,1].cpu().numpy(), bins=100, density=True, color=\"red\", alpha=0.5)\n",
    "plt.hist(cond_s[:,1,1].cpu().numpy(), bins=100, density=True, color=\"blue\", alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we sample from 2 i.i.d. multivariate Gaussians with 5\n",
    "dimensions each. The `1st` dimension of the `1st` Gaussian depends on the `0th`\n",
    "dimension (top plot) but the `1st` dimension of the `2nd` Gaussian is\n",
    "independent of the `0th` dimension of the `1st` Gaussian.\n",
    "\n",
    "In contrast, `batch_shape` represents the conditionally independent RVs. For\n",
    "example, the `MultivariateNormal` created above has a `batch_shape` of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(d.batch_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is because the mean vector and covariance matrix used to construct the\n",
    "distribution had shapes `(2,5)` and `(2,5,5)`, respectively. The batch\n",
    "dimensions don't have to be identically distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "m1 = torch.zeros(5)\n",
    "cov1 = torch.rand(5,5).abs()\n",
    "cov1 = torch.mm(cov1, cov1.t()) + 0.01 * torch.eye(5)\n",
    "m2 = torch.ones(5)\n",
    "cov2 = torch.rand(5,5).abs()\n",
    "cov2 = torch.mm(cov2, cov2.t()) + 0.01 * torch.eye(5)\n",
    "d = dist.MultivariateNormal(torch.stack([m1, m2]), torch.stack([cov1, cov2]))\n",
    "\n",
    "s = d.sample((100000,))\n",
    "print(s.shape)\n",
    "plt.hist(s[:, 0, 0].cpu().numpy(), bins=100, color=\"red\", alpha=0.5)\n",
    "plt.hist(s[:, 1, 0].cpu().numpy(), bins=100, color=\"blue\", alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, note that when we call the `sample` method we pass a `sample_shape`.\n",
    "This represents a tensor of i.i.d. samples drawn from the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(s[:50000, 0, 0].cpu().numpy(), bins=100, color=\"red\", alpha=0.5)\n",
    "plt.hist(s[50000:, 0, 0].cpu().numpy(), bins=100, color=\"blue\", alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of a sample is the concatenation of `event_shape`, `batch_shape`, and\n",
    "`sample_shape`. The shape of `log_prob`, on the other hand, is the concatenation\n",
    "of `sample_shape` and `batch_shape`, since the PDF is defined over entire\n",
    "events and not individual dimensions of them. As far as we can tell, pyro does\n",
    "not support getting conditional probabilities that are conditioned on some\n",
    "variables of a multivariate distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_shape = (100, 15)\n",
    "s = d.sample(sample_shape)\n",
    "assert(s.shape == sample_shape + d.batch_shape + d.event_shape)\n",
    "assert(d.log_prob(s).shape == sample_shape + d.batch_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've already created a distribution, we have a few mechanisms for\n",
    "changing the shapes of the distributions. Note that these mechanisms are not\n",
    "in-place, i.e. they return a new distribution.\n",
    "\n",
    "`to_event(n)` will move the `n` rightmost dimensions of the `batch_shape`\n",
    "(independent) to the left end of the `event_shape`, thus making them dependent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "default_d = dist.Normal(torch.zeros(3, 2, 5), 1)\n",
    "print(f\"batch_shape: {default_d.batch_shape};\\t\"\n",
    "      f\"event_shape: {default_d.event_shape}\")\n",
    "reshaped_d = default_d.to_event(1)\n",
    "print(f\"batch_shape: {reshaped_d.batch_shape};\\t\"\n",
    "      f\"event_shape: {reshaped_d.event_shape};\\t\")\n",
    "double_reshaped_d = default_d.to_event(2)\n",
    "print(f\"batch_shape: {double_reshaped_d.batch_shape};\\t\"\n",
    "      f\"event_shape: {double_reshaped_d.event_shape};\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no equivalent function that can convert dependent RVs to independent\n",
    "RVs. However, `expand()` can add new batch dimensions to the left of the\n",
    "`batch_shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def model1():\n",
    "    default_d = dist.Normal(torch.zeros(4,5), 1).to_event(1)\n",
    "    x = pyro.sample(\"x\", default_d)\n",
    "    y = pyro.sample(\"y\", default_d.expand([10, 4]))\n",
    "    z = pyro.sample(\"z\", default_d.expand([10] + [*default_d.batch_shape]))\n",
    "\n",
    "trace = poutine.trace(model1).get_trace()\n",
    "for k in ['x', 'y', 'z']:\n",
    "      print(f\"{k}: batch_shape = {trace.nodes[k]['fn'].batch_shape}\\t\"\n",
    "            f\"event_shape = {trace.nodes[k]['fn'].event_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the argument to `expand()` must be an iterable and its values must\n",
    "match the existing `batch_shape` at non-singleton dimensions. I.E. in the above\n",
    "example the right-most entry of the argument `[10, 4]` has to be `4` so that it\n",
    "matches the `-1th` entry of the distribution's `batch_shape`. This can be\n",
    "cumbersome, so instead you might want to use the context manager `pyro.plate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def model2():\n",
    "    default_d = dist.Normal(torch.zeros(4,5), 1).to_event(1)\n",
    "    x = pyro.sample(\"x\", default_d)\n",
    "    with pyro.plate(\"plate_0\", 10, dim=-2):\n",
    "        y = pyro.sample(\"y\", default_d)\n",
    "    with pyro.plate(\"plate_1\", 10, dim=-3):\n",
    "        z = pyro.sample(\"z\", default_d)\n",
    "    w =  pyro.sample(\"w\", default_d.expand([10, 4]))\n",
    "    with pyro.plate(\"plate_2\", 4, dim=-1):\n",
    "        a = pyro.sample(\"a\", default_d)\n",
    "\n",
    "\n",
    "\n",
    "trace = poutine.trace(model2).get_trace()\n",
    "for k in ['x', 'y', 'z', 'w', 'a']:\n",
    "      print(f\"{k}: batch_shape = {trace.nodes[k]['fn'].batch_shape}\\t\"\n",
    "            f\"event_shape = {trace.nodes[k]['fn'].event_shape}\")\n",
    "      print(trace.nodes[k]['cond_indep_stack'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we called `pyro.plate` with 3 arguments: 'name`, `size` and `dim`.\n",
    "\n",
    "`name` must be a unique identifier for each plate. `size` specifies the size of\n",
    "a new batch dimension, and `dim` the position at which to insert it. `dim` is\n",
    "always negatively indexed, and if it indicates a non-singleton dimension of\n",
    "`batch_shape`, then `size` must match that dimension. However, unlike `expand`,\n",
    "if we specify a `dim < -len(batch_shape)` then `pyro.plate` will automatically\n",
    "handle the rest of the shape, and even add new singleton dimensions if\n",
    "necessary.\n",
    "\n",
    "Another benefit is that any dimension declared independent with `pyro.plate`\n",
    "will have some conditional independence information added to its corresponding\n",
    "node. This information can be exploited by certain inference algorithms to run\n",
    "more efficiently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
