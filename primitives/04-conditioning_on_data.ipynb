{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior inference\n",
    "\n",
    "In this notebook we will address “learning” in the Bayesian context, which ends meaning updating the distributions of the parameters of a model, by conditioning on observed data.\n",
    "Additionally we will uses Monte Carlo methods to do inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coin fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import os\n",
    "import torch\n",
    "import torch.distributions.constraints as constraints\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyro\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import MCMC, NUTS\n",
    "import csv\n",
    "import numpy as np\n",
    "from src.graphs import coin_graph_multi, coin_graph_plate, coin_graph\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to and extend [the coin-tossing example from the pyro tutorial](http://pyro.ai/examples/svi_part_i.html).\n",
    "Our goal is to infer the fairness of the coin.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\operatorname{fairness} &\\sim \\operatorname{Beta}(10,10)\\\\\n",
    "\\operatorname{toss} &\\sim \\operatorname{Binom}(\\operatorname{fairness}).\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "# # define the hyperparameters that control the beta prior\n",
    "alpha0 = 10.0\n",
    "beta0 = 10.0\n",
    "\n",
    "def model(toss):\n",
    "    # sample f from the beta prior\n",
    "    \n",
    "    f = pyro.sample(\"latent_fairness\", dist.Beta(torch.tensor(alpha0), torch.tensor(beta0)))\n",
    "    # loop over the observed data\n",
    "    pyro.sample(\"toss\", dist.Bernoulli(f), obs=toss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualise the models  with [Graphical model diagrams](https://en.wikipedia.org/wiki/Graphical_model).\n",
    "This one comes out very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `pyro` one of the inference methods available to us is `MCMC`, the family of Markov Chain Monte Carlo methods. In these methods we use observations to update our prior estimates of the distributions over parameters, and we summarise that posterior distribution empirically - by specifying an algorithm which will simulate from that distribution. MCMC is overkill for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that I toss the coin one time and it comes up \"heads\", which we code as $\\operatorname{toss}=1.$\n",
    "Let us use pyro to condition on that single observation to update the distribution of the fairness.\n",
    "For this purpose we will simulate from the distribution $\\operatorname{fairness}|\\operatorname{toss}=1$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observation\n",
    "toss = torch.tensor([1.0])\n",
    "pyro.set_rng_seed(101)\n",
    "\n",
    "# sampler setup\n",
    "nuts_kernel = NUTS(model, jit_compile=True)\n",
    "mcmc = MCMC(nuts_kernel, num_samples=2000, warmup_steps=200)\n",
    "mcmc.run(toss)\n",
    "\n",
    "fairness_samples_1 = mcmc.get_samples()['latent_fairness'].cpu().numpy()\n",
    "\n",
    "print(f\"We now believe the fairness is {fairness_samples_1.mean():.3f} ± {fairness_samples_1.std()*1.96:.3f}\")\n",
    "\n",
    "# Plot the prior and posterior distributions\n",
    "prior_fairness = dist.Beta(alpha0, beta0)\n",
    "plt.hist(fairness_samples_1, bins=np.linspace(0,1,33), density=True, color=\"red\", label = \"1 observation posterior\", alpha=0.2)\n",
    "measurements = torch.linspace(0, 1, 1000)\n",
    "plt.plot(measurements, prior_fairness.log_prob(measurements).exp(), color=\"blue\", label=\"prior\");\n",
    "plt.legend(loc=\"upper left\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the model as given, we can only toss the coin once. But what if we have more data?\n",
    "Let us suppose that we flipped the coin 5 times and got heads each time. What should we now believe about our coint fairness?\n",
    "First we need to update the model to account for multiple observations,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\operatorname{fairness} &\\sim \\operatorname{Beta}(10,10)\\\\\n",
    "\\operatorname{toss_i} &\\sim \\operatorname{Binom}(\\operatorname{fairness}), i=1,\\dots,5.\n",
    "\\end{aligned}$$\n",
    "\n",
    "In code, we can write this as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "# # define the hyperparameters that control the beta prior\n",
    "alpha0 = 10.0\n",
    "beta0 = 10.0\n",
    "\n",
    "def model(tosses):\n",
    "    # sample f from the beta prior\n",
    "    \n",
    "    f = pyro.sample(\"latent_fairness\", dist.Beta(torch.tensor(alpha0), torch.tensor(beta0)))\n",
    "    # loop over the observed data\n",
    "    for i, toss in enumerate(tosses):\n",
    "        # observe datapoint i using the bernoulli likelihood\n",
    "        pyro.sample(\"toss_{}\".format(i), dist.Bernoulli(f), obs=toss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the dependency graph of this look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_graph_multi(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the MCMC inference algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tosses = torch.tensor([1,1,1,1,1], dtype=torch.float)\n",
    "pyro.set_rng_seed(102)\n",
    "\n",
    "nuts_kernel = NUTS(model, jit_compile=True)\n",
    "mcmc = MCMC(nuts_kernel, num_samples=2000, warmup_steps=200)\n",
    "mcmc.run(tosses)\n",
    "\n",
    "fairness_samples_5 = mcmc.get_samples()['latent_fairness'].cpu().numpy()\n",
    "\n",
    "print(f\"We now believe the fairness is {fairness_samples_5.mean():.3f} ± {fairness_samples_5.std()*1.96:.3f}\")\n",
    "\n",
    "prior_fairness = dist.Beta(alpha0, beta0)\n",
    "\n",
    "# Plot the prior and posterior distributions\n",
    "prior_fairness = dist.Beta(alpha0, beta0)\n",
    "plt.hist(fairness_samples_1, bins=np.linspace(0,1,33), density=True, color=\"red\", label = \"1 observation posterior\", alpha=0.2)\n",
    "plt.hist(fairness_samples_5, bins=np.linspace(0,1,33), density=True, color=\"red\", alpha = 0.4, label = \"5 observation posterior\")\n",
    "measurements = torch.linspace(0, 1, 1000)\n",
    "plt.plot(measurements, prior_fairness.log_prob(measurements).exp(), color=\"blue\", label=\"prior\");\n",
    "plt.legend(loc=\"upper left\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wait! Thanks to the new investment program, we have in fact tossed the coin 100 times in our new Google-funded robotic coin tossing laboratory, and recorded the data in a CSV file. Lets load that data up and print the first few observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some data from our csv file\n",
    "tosses = []\n",
    "with open('coin_tosses.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader) #skip header\n",
    "    for row in reader:\n",
    "        tosses.append(int(row[0]))\n",
    "tosses = torch.tensor(tosses, dtype=torch.float)\n",
    "print(tosses[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a hundred observations it starts to feel like using that plate notation might be nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "\n",
    "def model(tosses):\n",
    "    # # define the hyperparameters that control the beta prior\n",
    "    alpha0 = 10.0\n",
    "    beta0 = 10.0\n",
    "\n",
    "    # sample f from the beta prior\n",
    "    f = pyro.sample(\"latent_fairness\", dist.Beta(torch.tensor(alpha0), torch.tensor(beta0)))\n",
    "    # loop over the observed data\n",
    "    with pyro.plate(\"data\", tosses.shape[0]):\n",
    "        pyro.sample(\"tosses\", dist.Bernoulli(f), obs=tosses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_graph_plate(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(107)\n",
    "nuts_kernel = NUTS(model,\n",
    "    jit_compile=True\n",
    ")\n",
    "mcmc = MCMC(nuts_kernel, num_samples=2000, warmup_steps=200)\n",
    "mcmc.run(tosses)\n",
    "fairness_samples_100 = mcmc.get_samples()['latent_fairness'].cpu().numpy()\n",
    "\n",
    "print(f\"We now believe the fairness is {fairness_samples_100.mean():.3f} ± {fairness_samples_100.std()*1.96:.3f}\")\n",
    "\n",
    "# Plot the prior and posterior distributions\n",
    "prior_fairness = dist.Beta(alpha0, beta0)\n",
    "plt.hist(fairness_samples_1, bins=np.linspace(0,1,33), density=True, color=\"red\", label = \"1 observation posterior\", alpha=0.2)\n",
    "plt.hist(fairness_samples_5, bins=np.linspace(0,1,33), density=True, color=\"red\", alpha = 0.4, label = \"5 observation posterior\")\n",
    "plt.hist(fairness_samples_100, bins=np.linspace(0,1,33), density=True, color=\"red\", alpha = 1.0,label = \"100 observation posterior\")\n",
    "measurements = torch.linspace(0, 1, 1000)\n",
    "plt.plot(measurements, prior_fairness.log_prob(measurements).exp(), color=\"blue\", label=\"prior\");\n",
    "plt.legend(loc=\"upper left\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarising posterior with distributions\n",
    "\n",
    "So far we have been using MCMC to summarise the posterior distribution of samples.\n",
    "This method is well-understood and easy(ish) to configure, and is eventually unbiased with respect to most estimands. It is also the classic used in most textbooks.\n",
    "\n",
    "It also has downsides - for example, we have possibly quantified the output of a very simple model with a very large number of random samples.\n",
    "\n",
    "An alternative approach is to use variational inference, which summarises the posterior distributions with other distributions (this is in fact how they set it up in the tutorial).\n",
    "\n",
    "In Pyro terminology these approximation distributions are `guide` distributions and their parameters are captures by `pyro.param`s.\n",
    "We can access these in a default param store, which we get by calling `pyro.get_param_store()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# clear the param store in case we're in a REPL\n",
    "pyro.clear_param_store()\n",
    "\n",
    "def guide(tosses):\n",
    "    # register the two variational parameters with Pyro\n",
    "    # - both parameters will have initial value 15.0.\n",
    "    # - because we invoke constraints.positive, the optimizer\n",
    "    # will take gradients on the unconstrained parameters\n",
    "    # (which are related to the constrained parameters by a log)\n",
    "    alpha_q = pyro.param(\"alpha_q\", torch.tensor(15.0),\n",
    "                         constraint=constraints.positive)\n",
    "    beta_q = pyro.param(\"beta_q\", torch.tensor(15.0),\n",
    "                        constraint=constraints.positive)\n",
    "    # sample latent_fairness from the distribution Beta(alpha_q, beta_q)\n",
    "    pyro.sample(\"latent_fairness\", dist.Beta(alpha_q, beta_q))\n",
    "\n",
    "    \n",
    "def coin_variational_posterior(model, guide, tosses):\n",
    "    # setup the optimizer\n",
    "    n_steps = 200\n",
    "    adam_params = {\"lr\": 0.005, \"betas\": (0.90, 0.999)}\n",
    "    optimizer = Adam(adam_params)\n",
    "\n",
    "    # setup the inference algorithm\n",
    "    svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "    # do gradient steps\n",
    "    for step in range(n_steps):\n",
    "        svi.step(tosses)\n",
    "        if step % 100 == 0:\n",
    "            print('.', end='')\n",
    "\n",
    "# Do the inference\n",
    "coin_variational_posterior(model, guide, tosses)\n",
    "\n",
    "# # grab the learned variational parameters\n",
    "alpha_q = pyro.param(\"alpha_q\").item()\n",
    "beta_q = pyro.param(\"beta_q\").item()\n",
    "\n",
    "prior_fairness = dist.Beta(alpha0, beta0)\n",
    "posterior_fairness = dist.Beta(alpha_q, beta_q)\n",
    "inferred_mean = posterior_fairness.mean.item()\n",
    "inferred_std = sqrt(posterior_fairness.variance)\n",
    "\n",
    "print(\"\\nbased on the data and our prior belief, the fairness \" +\n",
    "      \"of the coin is %.3f +- %.3f\" % (inferred_mean, inferred_std))\n",
    "\n",
    "# Plot the prior and posterior distributions\n",
    "prior_fairness = dist.Beta(alpha0, beta0)\n",
    "plt.hist(fairness_samples_1, bins=np.linspace(0,1,33), density=True, color=\"red\", label = \"1 observation posterior\", alpha=0.2)\n",
    "plt.hist(fairness_samples_5, bins=np.linspace(0,1,33), density=True, color=\"red\", alpha = 0.4, label = \"5 observation posterior\")\n",
    "plt.hist(fairness_samples_100, bins=np.linspace(0,1,33), density=True, color=\"red\", alpha = 0.8, label = \"100 observation posterior\")\n",
    "measurements = torch.linspace(0, 1, 1000)\n",
    "plt.plot(measurements, prior_fairness.log_prob(measurements).exp(), color=\"blue\", label=\"prior\");\n",
    "plt.plot(measurements, posterior_fairness.log_prob(measurements).exp(), color=\"purple\", label=\"variational posterior\");\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "887521cc036c2176fc4c7c5fad660bcf5f9a9c2cadfc49851d28bf162a40070d"
  },
  "kernelspec": {
   "display_name": "Cadabra2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
