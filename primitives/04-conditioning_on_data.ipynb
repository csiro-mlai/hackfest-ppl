{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior inference\n",
    "\n",
    "In this notebook we will address “learning” in the Bayesian context, which ends meaning updating the distributions of the parameters of a model, by conditioning on observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coin fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import os\n",
    "import torch\n",
    "import torch.distributions.constraints as constraints\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import Predictive, MCMC, NUTS\n",
    "import csv\n",
    "import numpy as np\n",
    "from src.graphs import coin_graph_multi, coin_graph_plate, coin_graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to and extend [the coin-tossing example from the pyro tutorial](http://pyro.ai/examples/svi_part_i.html).\n",
    "Our goal is to infer the fairness of the coin.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\operatorname{fairness} &\\sim \\operatorname{Beta}(10,10)\\\\\n",
    "\\operatorname{toss} &\\sim \\operatorname{Binom}(\\operatorname{fairness}).\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "# # define the hyperparameters that control the beta prior\n",
    "alpha0 = 10.0\n",
    "beta0 = 10.0\n",
    "\n",
    "def model(toss):\n",
    "    # sample f from the beta prior\n",
    "    \n",
    "    f = pyro.sample(\"latent_fairness\", dist.Beta(torch.tensor(alpha0), torch.tensor(beta0)))\n",
    "    # loop over the observed data\n",
    "    pyro.sample(\"toss\", dist.Bernoulli(f), obs=toss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can diagram the dependency structure of that model thusly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that I toss the coin one time and it comes up \"heads\", which we code as $\\operatorname{toss}=1.$\n",
    "Let us use pyro to condition on that single observation to update the distribution of the fairness.\n",
    "For this purpose we will simulate from the distribution $\\operatorname{fairness}|\\operatorname{toss}=1$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observation\n",
    "toss = torch.tensor([1.0])\n",
    "pyro.set_rng_seed(101)\n",
    "\n",
    "# sampler setup\n",
    "nuts_kernel = NUTS(model, jit_compile=True)\n",
    "mcmc = MCMC(nuts_kernel, num_samples=2000, warmup_steps=200)\n",
    "mcmc.run(toss)\n",
    "\n",
    "# Plot the prior and posterior distributions\n",
    "prior_fairness = dist.Beta(alpha0, beta0)\n",
    "fairness_samples_1 = mcmc.get_samples()['latent_fairness'].cpu().numpy()\n",
    "plt.hist(fairness_samples_1, bins=np.linspace(0,1,33), density=True, color=\"red\", label = \"1 observation posterior\", alpha=0.2)\n",
    "measurements = torch.linspace(0, 1, 1000)\n",
    "plt.plot(measurements, prior_fairness.log_prob(measurements).exp(), color=\"blue\", label=\"prior\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I really hope the (red) posterior distribution came out pretty close to the (blue) prior distribution.\n",
    "\n",
    "Now, let us suppose that we flipped the coin 5 times and got heads, heads, tails, heads, tails. what does inference for that model look like?\n",
    "First we need to update the model to account for multiple observations,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\operatorname{fairness} &\\sim \\operatorname{Beta}(10,10)\\\\\n",
    "\\operatorname{toss_i} &\\sim \\operatorname{Binom}(\\operatorname{fairness}), i=1,\\dots,5.\n",
    "\\end{aligned}$$\n",
    "\n",
    "In code, we can write this as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "# # define the hyperparameters that control the beta prior\n",
    "alpha0 = 10.0\n",
    "beta0 = 10.0\n",
    "\n",
    "def model(tosses):\n",
    "    # sample f from the beta prior\n",
    "    \n",
    "    f = pyro.sample(\"latent_fairness\", dist.Beta(torch.tensor(alpha0), torch.tensor(beta0)))\n",
    "    # loop over the observed data\n",
    "    for i, toss in enumerate(tosses):\n",
    "        # observe datapoint i using the bernoulli likelihood\n",
    "        pyro.sample(\"toss_{}\".format(i), dist.Bernoulli(f), obs=toss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the dependency graph of this look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_graph_multi(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tosses = torch.tensor([1,1,0,1,0], dtype=torch.float)\n",
    "pyro.set_rng_seed(102)\n",
    "\n",
    "nuts_kernel = NUTS(model, jit_compile=True)\n",
    "mcmc = MCMC(nuts_kernel, num_samples=2000, warmup_steps=200)\n",
    "mcmc.run(tosses)\n",
    "\n",
    "prior_fairness = dist.Beta(alpha0, beta0)\n",
    "\n",
    "# Plot the prior and posterior distributions\n",
    "prior_fairness = dist.Beta(alpha0, beta0)\n",
    "fairness_samples_5 = mcmc.get_samples()['latent_fairness'].cpu().numpy()\n",
    "plt.hist(fairness_samples_1, bins=np.linspace(0,1,33), density=True, color=\"red\", label = \"1 observation posterior\", alpha=0.2)\n",
    "plt.hist(fairness_samples_5, bins=np.linspace(0,1,33), density=True, color=\"red\", alpha = 0.4, label = \"5 observation posterior\")\n",
    "measurements = torch.linspace(0, 1, 1000)\n",
    "plt.plot(measurements, prior_fairness.log_prob(measurements).exp(), color=\"blue\", label=\"prior\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have in fact tossed the coin 1000 times in our new Google-funded robotic coin tossing laboratory, and recorded the data in a CSV file. Lets load that up and print the first few observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some data from our csv file\n",
    "tosses = []\n",
    "with open('coin_tosses.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader) #skip header\n",
    "    for row in reader:\n",
    "        tosses.append(int(row[0]))\n",
    "tosses = torch.tensor(tosses, dtype=torch.float)\n",
    "print(tosses[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a thousand observations it starts to feel like using that plate notation might be nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "\n",
    "def model(tosses):\n",
    "    # # define the hyperparameters that control the beta prior\n",
    "    alpha0 = 10.0\n",
    "    beta0 = 10.0\n",
    "\n",
    "    # sample f from the beta prior\n",
    "    f = pyro.sample(\"latent_fairness\", dist.Beta(torch.tensor(alpha0), torch.tensor(beta0)))\n",
    "    # loop over the observed data\n",
    "    with pyro.plate(\"data\", tosses.shape[0]):\n",
    "        pyro.sample(\"tosses\", dist.Bernoulli(f), obs=toss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_graph_plate(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(103)\n",
    "nuts_kernel = NUTS(model, jit_compile=True)\n",
    "mcmc = MCMC(nuts_kernel, num_samples=2000, warmup_steps=200)\n",
    "mcmc.run(tosses)\n",
    "\n",
    "# Plot the prior and posterior distributions\n",
    "prior_fairness = dist.Beta(alpha0, beta0)\n",
    "fairness_samples_1000 = mcmc.get_samples()['latent_fairness'].cpu().numpy()\n",
    "plt.hist(fairness_samples_1, bins=np.linspace(0,1,33), density=True, color=\"red\", label = \"1 observation posterior\", alpha=0.2)\n",
    "plt.hist(fairness_samples_5, bins=np.linspace(0,1,33), density=True, color=\"red\", alpha = 0.4, label = \"5 observation posterior\")\n",
    "plt.hist(fairness_samples_1000, bins=np.linspace(0,1,33), density=True, color=\"red\", alpha = 0.8, label = \"1000 observation posterior\")\n",
    "measurements = torch.linspace(0, 1, 1000)\n",
    "plt.plot(measurements, prior_fairness.log_prob(measurements).exp(), color=\"blue\", label=\"prior\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is this telling us about the distribution of the fairness parameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We now believe the fairness is {latent_fairness_samples.mean():.3f} ± {latent_fairness_samples.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* http://pyro.ai/examples/bayesian_regression.html\n",
    "* http://pyro.ai/examples/bayesian_regression_ii.html\n",
    "* http://pyro.ai/examples/intro_part_i.html\n",
    "* http://pyro.ai/examples/intro_part_ii.html\n",
    "* http://pyro.ai/examples/effect_handlers.html\n",
    "* http://pyro.ai/examples/sir_hmc.html\n",
    "* https://docs.pyro.ai/en/1.7.0/poutine.html\n",
    "* http://pyro.ai/examples/mle_map.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An actual regression problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## archived SVI stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))\n",
    "fig.suptitle(\"Marginal Posterior density - Regression Coefficients\", fontsize=16)\n",
    "for i, ax in enumerate(axs.reshape(-1)):\n",
    "    site = sites[i]\n",
    "    sns.distplot(svi_samples[site], ax=ax, label=\"SVI (DiagNormal)\")\n",
    "    sns.distplot(hmc_samples[site], ax=ax, label=\"HMC\")\n",
    "    ax.set_title(site)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# # define how much of our data to use\n",
    "n_tosses = 1\n",
    "\n",
    "# clear the param store in case we're in a REPL\n",
    "pyro.clear_param_store()\n",
    "\n",
    "\n",
    "\n",
    "def guide(tosses):\n",
    "    # register the two variational parameters with Pyro\n",
    "    # - both parameters will have initial value 15.0.\n",
    "    # - because we invoke constraints.positive, the optimizer\n",
    "    # will take gradients on the unconstrained parameters\n",
    "    # (which are related to the constrained parameters by a log)\n",
    "    alpha_q = pyro.param(\"alpha_q\", torch.tensor(15.0),\n",
    "                         constraint=constraints.positive)\n",
    "    beta_q = pyro.param(\"beta_q\", torch.tensor(15.0),\n",
    "                        constraint=constraints.positive)\n",
    "    # sample latent_fairness from the distribution Beta(alpha_q, beta_q)\n",
    "    pyro.sample(\"latent_fairness\", dist.Beta(alpha_q, beta_q))\n",
    "\n",
    "    \n",
    "def basic_coin_posterior(model, guide, tosses):\n",
    "    # setup the optimizer\n",
    "    n_steps = 200\n",
    "    adam_params = {\"lr\": 0.005, \"betas\": (0.90, 0.999)}\n",
    "    optimizer = Adam(adam_params)\n",
    "\n",
    "    # setup the inference algorithm\n",
    "    svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "    # do gradient steps\n",
    "    for step in range(n_steps):\n",
    "        svi.step(tosses)\n",
    "        if step % 100 == 0:\n",
    "            print('.', end='')\n",
    "\n",
    "\n",
    "# # grab the learned variational parameters\n",
    "alpha_q = pyro.param(\"alpha_q\").item()\n",
    "beta_q = pyro.param(\"beta_q\").item()\n",
    "\n",
    "prior_fairness = dist.Beta(alpha0, beta0)\n",
    "posterior_fairness = dist.Beta(alpha_q, beta_q)\n",
    "inferred_mean = posterior_fairness.mean.item()\n",
    "inferred_std = sqrt(posterior_fairness.variance)\n",
    "\n",
    "print(\"\\nbased on the data and our prior belief, the fairness \" +\n",
    "      \"of the coin is %.3f +- %.3f\" % (inferred_mean, inferred_std))\n",
    "lb = 0\n",
    "ub = 1\n",
    "resolution = 1000\n",
    "measurements = torch.linspace(lb, ub, resolution)\n",
    "\n",
    "plt.plot(measurements, posterior_fairness.log_prob(measurements).exp(), color=\"blue\", label=\"posterior\");\n",
    "plt.plot(measurements, prior_fairness.log_prob(measurements).exp(), color=\"red\", label=\"prior\");\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "887521cc036c2176fc4c7c5fad660bcf5f9a9c2cadfc49851d28bf162a40070d"
  },
  "kernelspec": {
   "display_name": "Cadabra2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
