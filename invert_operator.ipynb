{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverting Hydrology with Neural Nets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import minimise_predictive_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import numpy as np\n",
    "import random\n",
    "import functools\n",
    "from math import sqrt\n",
    "from PIL import Image\n",
    "\n",
    "from typing import Any, Callable, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('png')\n",
    "# plt.rcParams.update({'figure.figsize': [12, 12]})\n",
    "# plt.rcParams.update({'figure.dpi': 200})\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "%matplotlib inline\n",
    "load_dotenv(find_dotenv());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Any, Dict, Tuple, Optional\n",
    "from math import ceil, sqrt\n",
    "\n",
    "import torch\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "import torch.fft as fft\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from fourierflow.nn_modules.loss import LpLoss\n",
    "from fourierflow.nn_modules.fourier_2d_generic import SimpleBlock2dGeneric\n",
    "from fourierflow.viz.heatmap import navier_stokes_heatmap, multi_heatmap\n",
    "from fourierflow.datastores.navier_stokes_h5 import NavierStokesH5InstDatastore\n",
    "from fourierflow.utils import resolve_path\n",
    "from fourierflow.optimizers import AdamWC\n",
    "from torch.linalg import vector_norm, matrix_norm\n",
    "\n",
    "from scipy.optimize import bisect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "# device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inversion by GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unregularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport fourierflow._infer\n",
    "infer = fourierflow._infer\n",
    "%aimport fourierflow.viz.heatmap\n",
    "heatmap = fourierflow.viz.heatmap\n",
    "\n",
    "# def plot_heatmap(model, i, loss, error, loss_fn, batch, pred, *args, **kwargs):\n",
    "#     err_heatmap = target - est\n",
    "#     fig = heatmap.multi_heatmap([target, est, err_heatmap], [\"target\", \"est\", \"error\"], *args, **kwargs)\n",
    "#     plt.show();\n",
    "#     # plt.savefig(f\"{i}.png\")\n",
    "#     plt.close(\"all\");\n",
    "\n",
    "def plot_heatmap(model, i, loss, error, loss_fn, batch, pred, *args, **kwargs):\n",
    "    target =  batch['latent'][0, :, :].cpu().numpy()\n",
    "    est =  model.latent[0, :, :].cpu().numpy()\n",
    "    err_heatmap = target - est\n",
    "\n",
    "    fig = heatmap.multi_heatmap(\n",
    "        [target, est, err_heatmap],\n",
    "        [\"Target\", \"Estimate\", \"Error\"], *args, **kwargs)\n",
    "    plt.savefig(f\"paper_ml4ps/inverse_{i}.png\", dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "    np.savez_compressed(f\"paper_ml4ps/inverse_{i}.npz\",\n",
    "        target=target,\n",
    "        est=est,\n",
    "        error=err_heatmap\n",
    "    )\n",
    "\n",
    "    plt.show();\n",
    "    plt.close(\"all\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(57)\n",
    "\n",
    "\n",
    "infer.main(\n",
    "    callback=plot_heatmap,\n",
    "    lr=0.005,\n",
    "    weight_decay=0.0,\n",
    "    n_iter=50,\n",
    "    check_int=5,\n",
    "    ds_args={\n",
    "        'data_path': '${FNO_DATA_ROOT}/navier-stokes/grf_forcing_mini.h5',\n",
    "        'ssr': 1,\n",
    "        'n_history': 2,\n",
    "        'n_horizon': 1,\n",
    "        'batch_size': 20,\n",
    "        'latent_key': 'f',\n",
    "        'forcing_key': '',\n",
    "        'param_key': '',\n",
    "        'n_workers': 0,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(57)\n",
    "\n",
    "infer.main(\n",
    "    callback=plot_heatmap,\n",
    "    lr=0.005,\n",
    "    weight_decay=0.0,\n",
    "    n_iter=50,\n",
    "    check_int=5,\n",
    "    ds_args={\n",
    "        'data_path': '${FNO_DATA_ROOT}/navier-stokes/grf_forcing_mini_1.h5',\n",
    "        'ssr': 1,\n",
    "        'n_history': 2,\n",
    "        'n_horizon': 1,\n",
    "        'batch_size': 20,\n",
    "        'latent_key': 'f',\n",
    "        'forcing_key': '',\n",
    "        'param_key': '',\n",
    "        'n_workers': 0,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = infer.main(\n",
    "#     callback=plot_heatmap, lr=0.1, weight_decay=0.0, n_iter=10, mapping='fourier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### diff penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_heatmap(model, i, loss, error, loss_fn, batch, pred, *args, **kwargs):\n",
    "    target =  batch['latent'][0, :, :].cpu().numpy()\n",
    "    est =  model.latent[0, :, :].cpu().numpy()\n",
    "    err_heatmap = target - est\n",
    "\n",
    "    fig = heatmap.multi_heatmap(\n",
    "        [target, est, err_heatmap],\n",
    "        [\"Target\", \"Estimate\", \"Error\"], *args, **kwargs)\n",
    "    plt.savefig(f\"paper_ml4ps/inverse_reg_{i}.png\", dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "    np.savez_compressed(f\"paper_ml4ps/inverse_reg_{i}.npz\",\n",
    "        target=target,\n",
    "        est=est,\n",
    "        error=err_heatmap\n",
    "    )\n",
    "    plt.show();\n",
    "    plt.close(\"all\");\n",
    "\n",
    "torch.manual_seed(59)\n",
    "\n",
    "model, fit, etc = infer.main(\n",
    "     ds_args={\n",
    "        'data_path': '${FNO_DATA_ROOT}/navier-stokes/grf_forcing_mini.h5',\n",
    "        'ssr': 1,\n",
    "        'n_history': 2,\n",
    "        'n_horizon': 1,\n",
    "        'batch_size': 20,\n",
    "        'latent_key': 'f',\n",
    "        'forcing_key': '',\n",
    "        'param_key': '',\n",
    "        'n_workers': 0,\n",
    "    },\n",
    "    callback=plot_heatmap, lr=0.01, weight_decay=0.00, n_iter=200, pen_1=30,check_int=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_heatmap(model, i, loss, error, loss_fn, batch, pred, *args, **kwargs):\n",
    "    target =  batch['latent'][0, :, :].cpu().numpy()\n",
    "    est =  model.latent[0, :, :].cpu().numpy()\n",
    "    err_heatmap = target - est\n",
    "\n",
    "    fig = heatmap.multi_heatmap(\n",
    "        [target, est, err_heatmap],\n",
    "        [\"Target\", \"Estimate\", \"Error\"], *args, **kwargs)\n",
    "    plt.savefig(f\"paper_ml4ps/inverse_reg_{i}.png\", dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "    np.savez_compressed(f\"paper_ml4ps/inverse_reg_{i}.npz\",\n",
    "        target=target,\n",
    "        est=est,\n",
    "        error=err_heatmap\n",
    "    )\n",
    "    plt.show();\n",
    "    plt.close(\"all\");\n",
    "\n",
    "torch.manual_seed(59)\n",
    "\n",
    "model, fit, etc = infer.main(\n",
    "     ds_args={\n",
    "        'data_path': '${FNO_DATA_ROOT}/navier-stokes/grf_forcing_mini_1.h5',\n",
    "        'ssr': 1,\n",
    "        'n_history': 2,\n",
    "        'n_horizon': 1,\n",
    "        'batch_size': 20,\n",
    "        'latent_key': 'f',\n",
    "        'forcing_key': '',\n",
    "        'param_key': '',\n",
    "        'n_workers': 0,\n",
    "    },\n",
    "    callback=plot_heatmap, lr=0.01, weight_decay=0.00, n_iter=200, pen_1=30,check_int=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## choosing regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fourierflow.datastores.navier_stokes_h5 import NavierStokesH5InstDatastore\n",
    "torch.manual_seed(60)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "n_batch = 20\n",
    "dims = (256, 256)\n",
    "\n",
    "def plot_heatmap(model, i, loss, error, loss_fn, batch, pred, *args, **kwargs):\n",
    "    target =  batch['latent'][0, :, :].cpu().numpy()\n",
    "    est =  model.latent[0, :, :].cpu().numpy()\n",
    "    err_heatmap = target - est\n",
    "\n",
    "    fig = heatmap.multi_heatmap(\n",
    "        [target, est, err_heatmap],\n",
    "        [\"Target\", \"Estimate\", \"Error\"], *args, **kwargs)\n",
    "    # plt.savefig(f\"paper_ml4ps/inverse_reg_{i}.png\",\n",
    "    #     dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "    # np.savez_compressed(f\"paper_ml4ps/inverse_reg_{i}.npz\",\n",
    "    #     target=target,\n",
    "    #     est=est,\n",
    "    #     error=err_heatmap\n",
    "    # )\n",
    "    plt.show();\n",
    "    plt.close(\"all\");\n",
    "\n",
    "\n",
    "datastore = NavierStokesH5InstDatastore(\n",
    "    '${FNO_DATA_ROOT}/navier-stokes/grf_forcing_mini_1.h5',\n",
    "    n_workers=0,\n",
    "    **{\n",
    "        'ssr': 1,\n",
    "        'n_history': 2,\n",
    "        'n_horizon': 1,\n",
    "        'batch_size': 20,\n",
    "        'latent_key': 'f',\n",
    "        'forcing_key': '',\n",
    "        'param_key': '',\n",
    "    }\n",
    ")\n",
    "dataloader = datastore.val_dataloader()\n",
    "batch = next(iter(dataloader))\n",
    "pp_state_dict = torch.load(\n",
    "        resolve_path('${SM_MODEL_DIR}/history_matching/adequate_checkpoint/fwd-epoch=19-step=26399-valid_loss=0.00000.ckpt'),\n",
    "        map_location=device\n",
    "    )\n",
    "process_predictor = SimpleBlock2dGeneric(\n",
    "    **{\n",
    "        'modes1': 16,\n",
    "        'width': 24,\n",
    "        'n_layers': 4,\n",
    "        'n_history': 2,\n",
    "        'param': False,\n",
    "        'forcing': False,\n",
    "        'latent': True,\n",
    "    }\n",
    ")\n",
    "process_predictor.load_state_dict(\n",
    "    pp_state_dict\n",
    ")\n",
    "model = infer.NaiveLatent(\n",
    "            process_predictor,\n",
    "            dims=dims,\n",
    "            n_batch=n_batch)\n",
    "model.to(device)\n",
    "npbatch = {}\n",
    "for (k,v,) in batch.items():\n",
    "    npbatch[k] = v.cpu().numpy()\n",
    "    batch[k] = v.to(device)\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.005,\n",
    "    weight_decay=0.0)\n",
    "loss_fn = nn.MSELoss().to(device)\n",
    "lambdas = [l**2 for l in range(0, 30)]\n",
    "\n",
    "relerrs = [\n",
    "    infer.fit(\n",
    "        batch,\n",
    "        model,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        n_iter=500,\n",
    "        check_int=5,\n",
    "        clip_val=None,\n",
    "        # callback=plot_heatmap,\n",
    "        # pen_0=pen_0,\n",
    "        pen_1=lambda_,\n",
    "        stop_on_truth=True,\n",
    "    )[2] for lambda_ in lambdas]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(repr(list(zip(lambdas, relerrs))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### longer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_heatmap(model, i, loss, error, loss_fn, batch, pred, *args, **kwargs):\n",
    "    target =  batch['latent'][0, :, :].cpu().numpy()\n",
    "    est =  model.latent[0, :, :].cpu().numpy()\n",
    "    err_heatmap = target - est\n",
    "\n",
    "    fig = heatmap.multi_heatmap(\n",
    "        [target, est, err_heatmap],\n",
    "        [\"Target\", \"Estimate\", \"Error\"], *args, **kwargs)\n",
    "    # plt.savefig(f\"paper_ml4ps/inverse_reg_{i}.png\", dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "    plt.show();\n",
    "    plt.close(\"all\");\n",
    "\n",
    "torch.manual_seed(59)\n",
    "\n",
    "\n",
    "infer.main(\n",
    "    fwd_state_dict_path= '${SM_MODEL_DIR}/history_matching/adequate_long_wide/history_matching/*/checkpoints/fwd-*.ckpt',\n",
    "    fwd_args={\n",
    "        'modes1': 16,\n",
    "        'width': 24,\n",
    "        'n_layers': 4,\n",
    "        'n_history': 10,\n",
    "        'param': False,\n",
    "        'forcing': False,\n",
    "        'latent': True,\n",
    "    },\n",
    "    ds_args={\n",
    "        'ssr': 1,\n",
    "        'n_history': 10,\n",
    "        'n_horizon': 1,\n",
    "        'batch_size': 20,\n",
    "        'latent_key': 'f',\n",
    "        'forcing_key': '',\n",
    "        'param_key': ''\n",
    "    },\n",
    "    callback=plot_heatmap,\n",
    "    lr=0.05, weight_decay=0.00, n_iter=10, pen_1=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas, relerrs = zip(*[(0, 1.6007785766982043), (1, 0.9997765660372001), (4, 0.9007331947795258), (9, 0.8229570081477463), (16, 0.8009129041645531), (25, 0.7878376155161687), (36, 0.7800267568964597), (49, 0.7752918051130337), (64, 0.7707536070233962), (81, 0.7663303353976826), (100, 0.764045919801015), (121, 0.7603965646406099), (144, 0.759816943122361), (169, 0.7553044588047234), (196, 0.7556437640875308), (225, 0.7545405463842202), (256, 0.7592473939734609), (289, 0.769563432528108), (324, 0.7806189493001515), (361, 0.7940838967495987), (400, 0.8070344769974499), (441, 0.8209803543025779), (484, 0.8324510644630306), (529, 0.8433933790498325), (576, 0.8549393999655539), (625, 0.8649656038660587), (676, 0.8746389898996968), (729, 0.8822984327202239), (784, 0.8900433311252656), (841, 0.8969211483359959)])\n",
    "\n",
    "fig = plt.plot(lambdas, relerrs, )\n",
    "plt.xlabel(r'$\\lambda$')\n",
    "plt.ylabel(\"relative error\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(0.75, 1)\n",
    "plt.savefig(f\"paper_ml4ps/inverse_reg_lambda.png\", dpi=300, bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = model.latent.cpu().detach().numpy()\n",
    "\n",
    "multi_heatmap([arr[0], arr[1], arr[2]], [\"1\", \"2\", \"3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer.fit(\n",
    "    batch,\n",
    "    model,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    n_iter=1000,\n",
    "    check_int=5,\n",
    "    clip_val=None,\n",
    "    # callback=plot_heatmap,\n",
    "    # pen_0=pen_0,\n",
    "    pen_1=30,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = model.latent.cpu().detach().numpy()\n",
    "for i in range(5):\n",
    "    est = arr[i]\n",
    "    target = npbatch['latent'][i]\n",
    "    err = est-target\n",
    "    multi_heatmap([est, target, err], [\"est\", \"target\", \"Error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = NavierStokesH5InstDatastore(\n",
    "    '${FNO_DATA_ROOT}/navier-stokes/grf_forcing_mini.h5',\n",
    "    **{\n",
    "        'ssr': 1,\n",
    "        'n_history': 2,\n",
    "        'n_horizon': 1,\n",
    "        'batch_size': 20,\n",
    "        'latent_key': 'f',\n",
    "        'forcing_key': '',\n",
    "        'param_key': ''\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### longer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_heatmap(model, i, loss, error, loss_fn, batch, pred, *args, **kwargs):\n",
    "    target =  batch['latent'][0, :, :].cpu().numpy()\n",
    "    est =  model.latent[0, :, :].cpu().numpy()\n",
    "    err_heatmap = target - est\n",
    "\n",
    "    fig = heatmap.multi_heatmap(\n",
    "        [target, est, err_heatmap],\n",
    "        [\"Target\", \"Estimate\", \"Error\"], *args, **kwargs)\n",
    "    # plt.savefig(f\"paper_ml4ps/inverse_reg_{i}.png\", dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "    plt.show();\n",
    "    plt.close(\"all\");\n",
    "\n",
    "torch.manual_seed(59)\n",
    "\n",
    "\n",
    "infer.main(\n",
    "    fwd_state_dict_path= '${SM_MODEL_DIR}/history_matching/adequate_long_wide/history_matching/*/checkpoints/fwd-*.ckpt',\n",
    "    fwd_args={\n",
    "        'modes1': 16,\n",
    "        'width': 24,\n",
    "        'n_layers': 4,\n",
    "        'n_history': 10,\n",
    "        'param': False,\n",
    "        'forcing': False,\n",
    "        'latent': True,\n",
    "    },\n",
    "    ds_args={\n",
    "        'ssr': 1,\n",
    "        'n_history': 10,\n",
    "        'n_horizon': 1,\n",
    "        'batch_size': 20,\n",
    "        'latent_key': 'f',\n",
    "        'forcing_key': '',\n",
    "        'param_key': ''\n",
    "    },\n",
    "    callback=plot_heatmap,\n",
    "    lr=0.05, weight_decay=0.00, n_iter=10, pen_1=30)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "887521cc036c2176fc4c7c5fad660bcf5f9a9c2cadfc49851d28bf162a40070d"
  },
  "kernelspec": {
   "display_name": "Cadabra2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
